{"metadata":{"kernelspec":{"language":"python","display_name":"Python 3","name":"python3"},"language_info":{"name":"python","version":"3.11.13","mimetype":"text/x-python","codemirror_mode":{"name":"ipython","version":3},"pygments_lexer":"ipython3","nbconvert_exporter":"python","file_extension":".py"},"kaggle":{"accelerator":"gpu","dataSources":[{"sourceId":13620630,"sourceType":"datasetVersion","datasetId":8646220}],"dockerImageVersionId":31154,"isInternetEnabled":true,"language":"python","sourceType":"notebook","isGpuEnabled":true}},"nbformat_minor":4,"nbformat":4,"cells":[{"cell_type":"code","source":"!apt-get update -qq","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-11-06T16:21:17.412142Z","iopub.execute_input":"2025-11-06T16:21:17.412378Z","iopub.status.idle":"2025-11-06T16:21:24.076590Z","shell.execute_reply.started":"2025-11-06T16:21:17.412356Z","shell.execute_reply":"2025-11-06T16:21:24.075550Z"}},"outputs":[{"name":"stdout","text":"W: Skipping acquire of configured file 'main/source/Sources' as repository 'https://r2u.stat.illinois.edu/ubuntu jammy InRelease' does not seem to provide it (sources.list entry misspelt?)\n","output_type":"stream"}],"execution_count":1},{"cell_type":"code","source":"!apt-get install -y ffmpeg libsndfile1","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-11-06T16:21:24.078367Z","iopub.execute_input":"2025-11-06T16:21:24.078635Z","iopub.status.idle":"2025-11-06T16:21:26.590799Z","shell.execute_reply.started":"2025-11-06T16:21:24.078611Z","shell.execute_reply":"2025-11-06T16:21:26.589798Z"}},"outputs":[{"name":"stdout","text":"Reading package lists... Done\nBuilding dependency tree... Done\nReading state information... Done\nlibsndfile1 is already the newest version (1.0.31-2ubuntu0.2).\nffmpeg is already the newest version (7:4.4.2-0ubuntu0.22.04.1).\n0 upgraded, 0 newly installed, 0 to remove and 171 not upgraded.\n","output_type":"stream"}],"execution_count":2},{"cell_type":"code","source":"!pip install --upgrade pip","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-11-06T16:21:26.594684Z","iopub.execute_input":"2025-11-06T16:21:26.594967Z","iopub.status.idle":"2025-11-06T16:21:32.785561Z","shell.execute_reply.started":"2025-11-06T16:21:26.594935Z","shell.execute_reply":"2025-11-06T16:21:32.784589Z"}},"outputs":[{"name":"stdout","text":"Requirement already satisfied: pip in /usr/local/lib/python3.11/dist-packages (24.1.2)\nCollecting pip\n  Downloading pip-25.3-py3-none-any.whl.metadata (4.7 kB)\nDownloading pip-25.3-py3-none-any.whl (1.8 MB)\n\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m1.8/1.8 MB\u001b[0m \u001b[31m10.6 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m \u001b[36m0:00:01\u001b[0m\n\u001b[?25hInstalling collected packages: pip\n  Attempting uninstall: pip\n    Found existing installation: pip 24.1.2\n    Uninstalling pip-24.1.2:\n      Successfully uninstalled pip-24.1.2\nSuccessfully installed pip-25.3\n","output_type":"stream"}],"execution_count":3},{"cell_type":"code","source":"!pip install pydantic==2.11.0 rich==13.7.1 pyarrow==19.0.0 --force-reinstall","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-11-06T16:21:32.788393Z","iopub.execute_input":"2025-11-06T16:21:32.788638Z","iopub.status.idle":"2025-11-06T16:21:55.618116Z","shell.execute_reply.started":"2025-11-06T16:21:32.788617Z","shell.execute_reply":"2025-11-06T16:21:55.617228Z"}},"outputs":[{"name":"stdout","text":"Collecting pydantic==2.11.0\n  Downloading pydantic-2.11.0-py3-none-any.whl.metadata (63 kB)\nCollecting rich==13.7.1\n  Downloading rich-13.7.1-py3-none-any.whl.metadata (18 kB)\nCollecting pyarrow==19.0.0\n  Downloading pyarrow-19.0.0-cp311-cp311-manylinux_2_28_x86_64.whl.metadata (3.3 kB)\nCollecting annotated-types>=0.6.0 (from pydantic==2.11.0)\n  Downloading annotated_types-0.7.0-py3-none-any.whl.metadata (15 kB)\nCollecting pydantic-core==2.33.0 (from pydantic==2.11.0)\n  Downloading pydantic_core-2.33.0-cp311-cp311-manylinux_2_17_x86_64.manylinux2014_x86_64.whl.metadata (6.8 kB)\nCollecting typing-extensions>=4.12.2 (from pydantic==2.11.0)\n  Downloading typing_extensions-4.15.0-py3-none-any.whl.metadata (3.3 kB)\nCollecting typing-inspection>=0.4.0 (from pydantic==2.11.0)\n  Downloading typing_inspection-0.4.2-py3-none-any.whl.metadata (2.6 kB)\nCollecting markdown-it-py>=2.2.0 (from rich==13.7.1)\n  Downloading markdown_it_py-4.0.0-py3-none-any.whl.metadata (7.3 kB)\nCollecting pygments<3.0.0,>=2.13.0 (from rich==13.7.1)\n  Downloading pygments-2.19.2-py3-none-any.whl.metadata (2.5 kB)\nCollecting mdurl~=0.1 (from markdown-it-py>=2.2.0->rich==13.7.1)\n  Downloading mdurl-0.1.2-py3-none-any.whl.metadata (1.6 kB)\nDownloading pydantic-2.11.0-py3-none-any.whl (442 kB)\nDownloading rich-13.7.1-py3-none-any.whl (240 kB)\nDownloading pyarrow-19.0.0-cp311-cp311-manylinux_2_28_x86_64.whl (42.1 MB)\n\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m42.1/42.1 MB\u001b[0m \u001b[31m35.9 MB/s\u001b[0m  \u001b[33m0:00:01\u001b[0mm0:00:01\u001b[0m00:01\u001b[0m\n\u001b[?25hDownloading pydantic_core-2.33.0-cp311-cp311-manylinux_2_17_x86_64.manylinux2014_x86_64.whl (2.0 MB)\n\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m2.0/2.0 MB\u001b[0m \u001b[31m63.6 MB/s\u001b[0m  \u001b[33m0:00:00\u001b[0m\n\u001b[?25hDownloading pygments-2.19.2-py3-none-any.whl (1.2 MB)\n\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m1.2/1.2 MB\u001b[0m \u001b[31m43.9 MB/s\u001b[0m  \u001b[33m0:00:00\u001b[0m\n\u001b[?25hDownloading annotated_types-0.7.0-py3-none-any.whl (13 kB)\nDownloading markdown_it_py-4.0.0-py3-none-any.whl (87 kB)\nDownloading mdurl-0.1.2-py3-none-any.whl (10.0 kB)\nDownloading typing_extensions-4.15.0-py3-none-any.whl (44 kB)\nDownloading typing_inspection-0.4.2-py3-none-any.whl (14 kB)\nInstalling collected packages: typing-extensions, pygments, pyarrow, mdurl, annotated-types, typing-inspection, pydantic-core, markdown-it-py, rich, pydantic\n\u001b[2K  Attempting uninstall: typing-extensions\n\u001b[2K    Found existing installation: typing_extensions 4.15.0\n\u001b[2K    Uninstalling typing_extensions-4.15.0:\n\u001b[2K      Successfully uninstalled typing_extensions-4.15.032m 0/10\u001b[0m [typing-extensions]\n\u001b[2K  Attempting uninstall: pygments━━━━━━━━━━━━\u001b[0m \u001b[32m 0/10\u001b[0m [typing-extensions]\n\u001b[2K    Found existing installation: Pygments 2.19.2 \u001b[32m 0/10\u001b[0m [typing-extensions]\n\u001b[2K    Uninstalling Pygments-2.19.2:━━━━━━━━━━━\u001b[0m \u001b[32m 0/10\u001b[0m [typing-extensions]\n\u001b[2K      Successfully uninstalled Pygments-2.19.2━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m 1/10\u001b[0m [pygments]\n\u001b[2K  Attempting uninstall: pyarrow━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m 1/10\u001b[0m [pygments]\n\u001b[2K    Found existing installation: pyarrow 19.0.1━━━━━━━━━━━━━━━\u001b[0m \u001b[32m 1/10\u001b[0m [pygments]\n\u001b[2K    Uninstalling pyarrow-19.0.1:━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m 1/10\u001b[0m [pygments]\n\u001b[2K      Successfully uninstalled pyarrow-19.0.1━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m 2/10\u001b[0m [pyarrow]\n\u001b[2K  Attempting uninstall: mdurlm\u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m 2/10\u001b[0m [pyarrow]\n\u001b[2K    Found existing installation: mdurl 0.1.2━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m 2/10\u001b[0m [pyarrow]\n\u001b[2K    Uninstalling mdurl-0.1.2:0m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m 2/10\u001b[0m [pyarrow]\n\u001b[2K      Successfully uninstalled mdurl-0.1.2━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m 3/10\u001b[0m [mdurl]\n\u001b[2K  Attempting uninstall: annotated-types━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m 3/10\u001b[0m [mdurl]\n\u001b[2K    Found existing installation: annotated-types 0.7.0━━━━━━━━\u001b[0m \u001b[32m 3/10\u001b[0m [mdurl]\n\u001b[2K    Uninstalling annotated-types-0.7.0:━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m 3/10\u001b[0m [mdurl]\n\u001b[2K      Successfully uninstalled annotated-types-0.7.0━━━━━━━━━━\u001b[0m \u001b[32m 3/10\u001b[0m [mdurl]\n\u001b[2K  Attempting uninstall: typing-inspection━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m 3/10\u001b[0m [mdurl]\n\u001b[2K    Found existing installation: typing-inspection 0.4.1━━━━━━\u001b[0m \u001b[32m 3/10\u001b[0m [mdurl]\n\u001b[2K    Uninstalling typing-inspection-0.4.1:━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m 3/10\u001b[0m [mdurl]\n\u001b[2K      Successfully uninstalled typing-inspection-0.4.1━━━━━━━━\u001b[0m \u001b[32m 3/10\u001b[0m [mdurl]\n\u001b[2K  Attempting uninstall: pydantic-core━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m 3/10\u001b[0m [mdurl]\n\u001b[2K    Found existing installation: pydantic_core 2.37.2━━━━━━━━━\u001b[0m \u001b[32m 3/10\u001b[0m [mdurl]\n\u001b[2K    Uninstalling pydantic_core-2.37.2:━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m 3/10\u001b[0m [mdurl]\n\u001b[2K      Successfully uninstalled pydantic_core-2.37.2━━━━━━━━━━━━━━━\u001b[0m \u001b[32m 6/10\u001b[0m [pydantic-core]\n\u001b[2K  Attempting uninstall: markdown-it-py\u001b[0m\u001b[90m━━━━━━━━━━━━━━━\u001b[0m \u001b[32m 6/10\u001b[0m [pydantic-core]\n\u001b[2K    Found existing installation: markdown-it-py 4.0.0━━━━━━━━━\u001b[0m \u001b[32m 6/10\u001b[0m [pydantic-core]\n\u001b[2K    Uninstalling markdown-it-py-4.0.0:\u001b[0m\u001b[90m━━━━━━━━━━━━━━━\u001b[0m \u001b[32m 6/10\u001b[0m [pydantic-core]\n\u001b[2K      Successfully uninstalled markdown-it-py-4.0.0[90m━━━━━━━━━━━\u001b[0m \u001b[32m 7/10\u001b[0m [markdown-it-py]\n\u001b[2K  Attempting uninstall: rich━━━━\u001b[0m\u001b[90m╺\u001b[0m\u001b[90m━━━━━━━━━━━\u001b[0m \u001b[32m 7/10\u001b[0m [markdown-it-py]\n\u001b[2K    Found existing installation: rich 14.1.00m\u001b[90m━━━━━━━━━━━\u001b[0m \u001b[32m 7/10\u001b[0m [markdown-it-py]\n\u001b[2K    Uninstalling rich-14.1.0:━━━\u001b[0m\u001b[90m╺\u001b[0m\u001b[90m━━━━━━━━━━━\u001b[0m \u001b[32m 7/10\u001b[0m [markdown-it-py]\n\u001b[2K      Successfully uninstalled rich-14.1.00m\u001b[90m╺\u001b[0m\u001b[90m━━━━━━━\u001b[0m \u001b[32m 8/10\u001b[0m [rich]it-py]\n\u001b[2K  Attempting uninstall: pydantic━━━━━━━━\u001b[0m\u001b[90m╺\u001b[0m\u001b[90m━━━━━━━\u001b[0m \u001b[32m 8/10\u001b[0m [rich]\n\u001b[2K    Found existing installation: pydantic 2.12.0a1\u001b[90m━━━━━━━\u001b[0m \u001b[32m 8/10\u001b[0m [rich]\n\u001b[2K    Uninstalling pydantic-2.12.0a1:━\u001b[0m\u001b[90m╺\u001b[0m\u001b[90m━━━━━━━\u001b[0m \u001b[32m 8/10\u001b[0m [rich]\n\u001b[2K      Successfully uninstalled pydantic-2.12.0a10m\u001b[90m━━━━━━━\u001b[0m \u001b[32m 8/10\u001b[0m [rich]\n\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m10/10\u001b[0m [pydantic]/10\u001b[0m [pydantic]\n\u001b[1A\u001b[2K\u001b[31mERROR: pip's dependency resolver does not currently take into account all the packages that are installed. This behaviour is the source of the following dependency conflicts.\nbigframes 2.12.0 requires google-cloud-bigquery-storage<3.0.0,>=2.30.0, which is not installed.\nsigstore-models 0.0.5 requires pydantic>=2.11.7, but you have pydantic 2.11.0 which is incompatible.\ndatasets 4.1.1 requires pyarrow>=21.0.0, but you have pyarrow 19.0.0 which is incompatible.\nonnx 1.18.0 requires protobuf>=4.25.1, but you have protobuf 3.20.3 which is incompatible.\ndopamine-rl 4.1.2 requires gymnasium>=1.0.0, but you have gymnasium 0.29.0 which is incompatible.\nbigframes 2.12.0 requires google-cloud-bigquery[bqstorage,pandas]>=3.31.0, but you have google-cloud-bigquery 3.25.0 which is incompatible.\nibis-framework 9.5.0 requires toolz<1,>=0.11, but you have toolz 1.0.0 which is incompatible.\ntokenizers 0.21.2 requires huggingface-hub<1.0,>=0.16.4, but you have huggingface-hub 1.0.0rc2 which is incompatible.\nthinc 8.3.6 requires numpy<3.0.0,>=2.0.0, but you have numpy 1.26.4 which is incompatible.\ntorch 2.6.0+cu124 requires nvidia-cublas-cu12==12.4.5.8; platform_system == \"Linux\" and platform_machine == \"x86_64\", but you have nvidia-cublas-cu12 12.5.3.2 which is incompatible.\ntorch 2.6.0+cu124 requires nvidia-cuda-cupti-cu12==12.4.127; platform_system == \"Linux\" and platform_machine == \"x86_64\", but you have nvidia-cuda-cupti-cu12 12.5.82 which is incompatible.\ntorch 2.6.0+cu124 requires nvidia-cuda-nvrtc-cu12==12.4.127; platform_system == \"Linux\" and platform_machine == \"x86_64\", but you have nvidia-cuda-nvrtc-cu12 12.5.82 which is incompatible.\ntorch 2.6.0+cu124 requires nvidia-cuda-runtime-cu12==12.4.127; platform_system == \"Linux\" and platform_machine == \"x86_64\", but you have nvidia-cuda-runtime-cu12 12.5.82 which is incompatible.\ntorch 2.6.0+cu124 requires nvidia-cudnn-cu12==9.1.0.70; platform_system == \"Linux\" and platform_machine == \"x86_64\", but you have nvidia-cudnn-cu12 9.3.0.75 which is incompatible.\ntorch 2.6.0+cu124 requires nvidia-cufft-cu12==11.2.1.3; platform_system == \"Linux\" and platform_machine == \"x86_64\", but you have nvidia-cufft-cu12 11.2.3.61 which is incompatible.\ntorch 2.6.0+cu124 requires nvidia-curand-cu12==10.3.5.147; platform_system == \"Linux\" and platform_machine == \"x86_64\", but you have nvidia-curand-cu12 10.3.6.82 which is incompatible.\ntorch 2.6.0+cu124 requires nvidia-cusolver-cu12==11.6.1.9; platform_system == \"Linux\" and platform_machine == \"x86_64\", but you have nvidia-cusolver-cu12 11.6.3.83 which is incompatible.\ntorch 2.6.0+cu124 requires nvidia-cusparse-cu12==12.3.1.170; platform_system == \"Linux\" and platform_machine == \"x86_64\", but you have nvidia-cusparse-cu12 12.5.1.3 which is incompatible.\ntorch 2.6.0+cu124 requires nvidia-nvjitlink-cu12==12.4.127; platform_system == \"Linux\" and platform_machine == \"x86_64\", but you have nvidia-nvjitlink-cu12 12.5.82 which is incompatible.\ncudf-polars-cu12 25.6.0 requires pylibcudf-cu12==25.6.*, but you have pylibcudf-cu12 25.2.2 which is incompatible.\nmdit-py-plugins 0.4.2 requires markdown-it-py<4.0.0,>=1.0.0, but you have markdown-it-py 4.0.0 which is incompatible.\npydrive2 1.21.3 requires cryptography<44, but you have cryptography 46.0.1 which is incompatible.\npydrive2 1.21.3 requires pyOpenSSL<=24.2.1,>=19.1.0, but you have pyopenssl 25.3.0 which is incompatible.\npandas-gbq 0.29.2 requires google-api-core<3.0.0,>=2.10.2, but you have google-api-core 1.34.1 which is incompatible.\ntransformers 4.53.3 requires huggingface-hub<1.0,>=0.30.0, but you have huggingface-hub 1.0.0rc2 which is incompatible.\ngcsfs 2025.3.0 requires fsspec==2025.3.0, but you have fsspec 2025.9.0 which is incompatible.\u001b[0m\u001b[31m\n\u001b[0mSuccessfully installed annotated-types-0.7.0 markdown-it-py-4.0.0 mdurl-0.1.2 pyarrow-19.0.0 pydantic-2.11.0 pydantic-core-2.33.0 pygments-2.19.2 rich-13.7.1 typing-extensions-4.15.0 typing-inspection-0.4.2\n","output_type":"stream"}],"execution_count":4},{"cell_type":"code","source":"!pip install transformers datasets accelerate","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-11-06T16:21:55.619071Z","iopub.execute_input":"2025-11-06T16:21:55.619367Z","iopub.status.idle":"2025-11-06T16:23:12.012199Z","shell.execute_reply.started":"2025-11-06T16:21:55.619331Z","shell.execute_reply":"2025-11-06T16:23:12.011259Z"}},"outputs":[{"name":"stdout","text":"Requirement already satisfied: transformers in /usr/local/lib/python3.11/dist-packages (4.53.3)\nRequirement already satisfied: datasets in /usr/local/lib/python3.11/dist-packages (4.1.1)\nRequirement already satisfied: accelerate in /usr/local/lib/python3.11/dist-packages (1.9.0)\nRequirement already satisfied: filelock in /usr/local/lib/python3.11/dist-packages (from transformers) (3.19.1)\nCollecting huggingface-hub<1.0,>=0.30.0 (from transformers)\n  Downloading huggingface_hub-0.36.0-py3-none-any.whl.metadata (14 kB)\nRequirement already satisfied: numpy>=1.17 in /usr/local/lib/python3.11/dist-packages (from transformers) (1.26.4)\nRequirement already satisfied: packaging>=20.0 in /usr/local/lib/python3.11/dist-packages (from transformers) (25.0)\nRequirement already satisfied: pyyaml>=5.1 in /usr/local/lib/python3.11/dist-packages (from transformers) (6.0.3)\nRequirement already satisfied: regex!=2019.12.17 in /usr/local/lib/python3.11/dist-packages (from transformers) (2025.9.18)\nRequirement already satisfied: requests in /usr/local/lib/python3.11/dist-packages (from transformers) (2.32.5)\nRequirement already satisfied: tokenizers<0.22,>=0.21 in /usr/local/lib/python3.11/dist-packages (from transformers) (0.21.2)\nRequirement already satisfied: safetensors>=0.4.3 in /usr/local/lib/python3.11/dist-packages (from transformers) (0.5.3)\nRequirement already satisfied: tqdm>=4.27 in /usr/local/lib/python3.11/dist-packages (from transformers) (4.67.1)\nRequirement already satisfied: fsspec>=2023.5.0 in /usr/local/lib/python3.11/dist-packages (from huggingface-hub<1.0,>=0.30.0->transformers) (2025.9.0)\nRequirement already satisfied: typing-extensions>=3.7.4.3 in /usr/local/lib/python3.11/dist-packages (from huggingface-hub<1.0,>=0.30.0->transformers) (4.15.0)\nRequirement already satisfied: hf-xet<2.0.0,>=1.1.3 in /usr/local/lib/python3.11/dist-packages (from huggingface-hub<1.0,>=0.30.0->transformers) (1.1.10)\nCollecting pyarrow>=21.0.0 (from datasets)\n  Downloading pyarrow-22.0.0-cp311-cp311-manylinux_2_28_x86_64.whl.metadata (3.2 kB)\nRequirement already satisfied: dill<0.4.1,>=0.3.0 in /usr/local/lib/python3.11/dist-packages (from datasets) (0.4.0)\nRequirement already satisfied: pandas in /usr/local/lib/python3.11/dist-packages (from datasets) (2.2.3)\nRequirement already satisfied: xxhash in /usr/local/lib/python3.11/dist-packages (from datasets) (3.5.0)\nRequirement already satisfied: multiprocess<0.70.17 in /usr/local/lib/python3.11/dist-packages (from datasets) (0.70.16)\nRequirement already satisfied: aiohttp!=4.0.0a0,!=4.0.0a1 in /usr/local/lib/python3.11/dist-packages (from fsspec[http]<=2025.9.0,>=2023.1.0->datasets) (3.12.15)\nRequirement already satisfied: psutil in /usr/local/lib/python3.11/dist-packages (from accelerate) (7.1.0)\nRequirement already satisfied: torch>=2.0.0 in /usr/local/lib/python3.11/dist-packages (from accelerate) (2.6.0+cu124)\nRequirement already satisfied: mkl_fft in /usr/local/lib/python3.11/dist-packages (from numpy>=1.17->transformers) (1.3.8)\nRequirement already satisfied: mkl_random in /usr/local/lib/python3.11/dist-packages (from numpy>=1.17->transformers) (1.2.4)\nRequirement already satisfied: mkl_umath in /usr/local/lib/python3.11/dist-packages (from numpy>=1.17->transformers) (0.1.1)\nRequirement already satisfied: mkl in /usr/local/lib/python3.11/dist-packages (from numpy>=1.17->transformers) (2025.2.0)\nRequirement already satisfied: tbb4py in /usr/local/lib/python3.11/dist-packages (from numpy>=1.17->transformers) (2022.2.0)\nRequirement already satisfied: mkl-service in /usr/local/lib/python3.11/dist-packages (from numpy>=1.17->transformers) (2.4.1)\nRequirement already satisfied: aiohappyeyeballs>=2.5.0 in /usr/local/lib/python3.11/dist-packages (from aiohttp!=4.0.0a0,!=4.0.0a1->fsspec[http]<=2025.9.0,>=2023.1.0->datasets) (2.6.1)\nRequirement already satisfied: aiosignal>=1.4.0 in /usr/local/lib/python3.11/dist-packages (from aiohttp!=4.0.0a0,!=4.0.0a1->fsspec[http]<=2025.9.0,>=2023.1.0->datasets) (1.4.0)\nRequirement already satisfied: attrs>=17.3.0 in /usr/local/lib/python3.11/dist-packages (from aiohttp!=4.0.0a0,!=4.0.0a1->fsspec[http]<=2025.9.0,>=2023.1.0->datasets) (25.3.0)\nRequirement already satisfied: frozenlist>=1.1.1 in /usr/local/lib/python3.11/dist-packages (from aiohttp!=4.0.0a0,!=4.0.0a1->fsspec[http]<=2025.9.0,>=2023.1.0->datasets) (1.7.0)\nRequirement already satisfied: multidict<7.0,>=4.5 in /usr/local/lib/python3.11/dist-packages (from aiohttp!=4.0.0a0,!=4.0.0a1->fsspec[http]<=2025.9.0,>=2023.1.0->datasets) (6.6.4)\nRequirement already satisfied: propcache>=0.2.0 in /usr/local/lib/python3.11/dist-packages (from aiohttp!=4.0.0a0,!=4.0.0a1->fsspec[http]<=2025.9.0,>=2023.1.0->datasets) (0.3.2)\nRequirement already satisfied: yarl<2.0,>=1.17.0 in /usr/local/lib/python3.11/dist-packages (from aiohttp!=4.0.0a0,!=4.0.0a1->fsspec[http]<=2025.9.0,>=2023.1.0->datasets) (1.20.1)\nRequirement already satisfied: idna>=2.0 in /usr/local/lib/python3.11/dist-packages (from yarl<2.0,>=1.17.0->aiohttp!=4.0.0a0,!=4.0.0a1->fsspec[http]<=2025.9.0,>=2023.1.0->datasets) (3.10)\nRequirement already satisfied: charset_normalizer<4,>=2 in /usr/local/lib/python3.11/dist-packages (from requests->transformers) (3.4.3)\nRequirement already satisfied: urllib3<3,>=1.21.1 in /usr/local/lib/python3.11/dist-packages (from requests->transformers) (2.5.0)\nRequirement already satisfied: certifi>=2017.4.17 in /usr/local/lib/python3.11/dist-packages (from requests->transformers) (2025.8.3)\nRequirement already satisfied: networkx in /usr/local/lib/python3.11/dist-packages (from torch>=2.0.0->accelerate) (3.5)\nRequirement already satisfied: jinja2 in /usr/local/lib/python3.11/dist-packages (from torch>=2.0.0->accelerate) (3.1.6)\nCollecting nvidia-cuda-nvrtc-cu12==12.4.127 (from torch>=2.0.0->accelerate)\n  Downloading nvidia_cuda_nvrtc_cu12-12.4.127-py3-none-manylinux2014_x86_64.whl.metadata (1.5 kB)\nCollecting nvidia-cuda-runtime-cu12==12.4.127 (from torch>=2.0.0->accelerate)\n  Downloading nvidia_cuda_runtime_cu12-12.4.127-py3-none-manylinux2014_x86_64.whl.metadata (1.5 kB)\nCollecting nvidia-cuda-cupti-cu12==12.4.127 (from torch>=2.0.0->accelerate)\n  Downloading nvidia_cuda_cupti_cu12-12.4.127-py3-none-manylinux2014_x86_64.whl.metadata (1.6 kB)\nCollecting nvidia-cudnn-cu12==9.1.0.70 (from torch>=2.0.0->accelerate)\n  Downloading nvidia_cudnn_cu12-9.1.0.70-py3-none-manylinux2014_x86_64.whl.metadata (1.6 kB)\nCollecting nvidia-cublas-cu12==12.4.5.8 (from torch>=2.0.0->accelerate)\n  Downloading nvidia_cublas_cu12-12.4.5.8-py3-none-manylinux2014_x86_64.whl.metadata (1.5 kB)\nCollecting nvidia-cufft-cu12==11.2.1.3 (from torch>=2.0.0->accelerate)\n  Downloading nvidia_cufft_cu12-11.2.1.3-py3-none-manylinux2014_x86_64.whl.metadata (1.5 kB)\nCollecting nvidia-curand-cu12==10.3.5.147 (from torch>=2.0.0->accelerate)\n  Downloading nvidia_curand_cu12-10.3.5.147-py3-none-manylinux2014_x86_64.whl.metadata (1.5 kB)\nCollecting nvidia-cusolver-cu12==11.6.1.9 (from torch>=2.0.0->accelerate)\n  Downloading nvidia_cusolver_cu12-11.6.1.9-py3-none-manylinux2014_x86_64.whl.metadata (1.6 kB)\nCollecting nvidia-cusparse-cu12==12.3.1.170 (from torch>=2.0.0->accelerate)\n  Downloading nvidia_cusparse_cu12-12.3.1.170-py3-none-manylinux2014_x86_64.whl.metadata (1.6 kB)\nRequirement already satisfied: nvidia-cusparselt-cu12==0.6.2 in /usr/local/lib/python3.11/dist-packages (from torch>=2.0.0->accelerate) (0.6.2)\nRequirement already satisfied: nvidia-nccl-cu12==2.21.5 in /usr/local/lib/python3.11/dist-packages (from torch>=2.0.0->accelerate) (2.21.5)\nRequirement already satisfied: nvidia-nvtx-cu12==12.4.127 in /usr/local/lib/python3.11/dist-packages (from torch>=2.0.0->accelerate) (12.4.127)\nCollecting nvidia-nvjitlink-cu12==12.4.127 (from torch>=2.0.0->accelerate)\n  Downloading nvidia_nvjitlink_cu12-12.4.127-py3-none-manylinux2014_x86_64.whl.metadata (1.5 kB)\nRequirement already satisfied: triton==3.2.0 in /usr/local/lib/python3.11/dist-packages (from torch>=2.0.0->accelerate) (3.2.0)\nRequirement already satisfied: sympy==1.13.1 in /usr/local/lib/python3.11/dist-packages (from torch>=2.0.0->accelerate) (1.13.1)\nRequirement already satisfied: mpmath<1.4,>=1.1.0 in /usr/local/lib/python3.11/dist-packages (from sympy==1.13.1->torch>=2.0.0->accelerate) (1.3.0)\nRequirement already satisfied: MarkupSafe>=2.0 in /usr/local/lib/python3.11/dist-packages (from jinja2->torch>=2.0.0->accelerate) (3.0.2)\nRequirement already satisfied: intel-openmp<2026,>=2024 in /usr/local/lib/python3.11/dist-packages (from mkl->numpy>=1.17->transformers) (2024.2.0)\nRequirement already satisfied: tbb==2022.* in /usr/local/lib/python3.11/dist-packages (from mkl->numpy>=1.17->transformers) (2022.2.0)\nRequirement already satisfied: intel-cmplr-lib-ur==2024.2.0 in /usr/local/lib/python3.11/dist-packages (from intel-openmp<2026,>=2024->mkl->numpy>=1.17->transformers) (2024.2.0)\nRequirement already satisfied: tcmlib==1.* in /usr/local/lib/python3.11/dist-packages (from tbb==2022.*->mkl->numpy>=1.17->transformers) (1.4.0)\nRequirement already satisfied: intel-cmplr-lib-rt in /usr/local/lib/python3.11/dist-packages (from mkl_umath->numpy>=1.17->transformers) (2024.2.0)\nRequirement already satisfied: python-dateutil>=2.8.2 in /usr/local/lib/python3.11/dist-packages (from pandas->datasets) (2.9.0.post0)\nRequirement already satisfied: pytz>=2020.1 in /usr/local/lib/python3.11/dist-packages (from pandas->datasets) (2025.2)\nRequirement already satisfied: tzdata>=2022.7 in /usr/local/lib/python3.11/dist-packages (from pandas->datasets) (2025.2)\nRequirement already satisfied: six>=1.5 in /usr/local/lib/python3.11/dist-packages (from python-dateutil>=2.8.2->pandas->datasets) (1.17.0)\nDownloading huggingface_hub-0.36.0-py3-none-any.whl (566 kB)\n\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m566.1/566.1 kB\u001b[0m \u001b[31m7.5 MB/s\u001b[0m  \u001b[33m0:00:00\u001b[0m\n\u001b[?25hDownloading pyarrow-22.0.0-cp311-cp311-manylinux_2_28_x86_64.whl (47.7 MB)\n\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m47.7/47.7 MB\u001b[0m \u001b[31m81.0 MB/s\u001b[0m  \u001b[33m0:00:00\u001b[0m6m0:00:01\u001b[0m00:01\u001b[0m\n\u001b[?25hDownloading nvidia_cublas_cu12-12.4.5.8-py3-none-manylinux2014_x86_64.whl (363.4 MB)\n\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m363.4/363.4 MB\u001b[0m \u001b[31m66.0 MB/s\u001b[0m  \u001b[33m0:00:04\u001b[0mm0:00:01\u001b[0m00:01\u001b[0m\n\u001b[?25hDownloading nvidia_cuda_cupti_cu12-12.4.127-py3-none-manylinux2014_x86_64.whl (13.8 MB)\n\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m13.8/13.8 MB\u001b[0m \u001b[31m96.0 MB/s\u001b[0m  \u001b[33m0:00:00\u001b[0m\n\u001b[?25hDownloading nvidia_cuda_nvrtc_cu12-12.4.127-py3-none-manylinux2014_x86_64.whl (24.6 MB)\n\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m24.6/24.6 MB\u001b[0m \u001b[31m96.8 MB/s\u001b[0m  \u001b[33m0:00:00\u001b[0m6m0:00:01\u001b[0m\n\u001b[?25hDownloading nvidia_cuda_runtime_cu12-12.4.127-py3-none-manylinux2014_x86_64.whl (883 kB)\n\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m883.7/883.7 kB\u001b[0m \u001b[31m33.0 MB/s\u001b[0m  \u001b[33m0:00:00\u001b[0m\n\u001b[?25hDownloading nvidia_cudnn_cu12-9.1.0.70-py3-none-manylinux2014_x86_64.whl (664.8 MB)\n\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m664.8/664.8 MB\u001b[0m \u001b[31m44.7 MB/s\u001b[0m  \u001b[33m0:00:08\u001b[0mm0:00:01\u001b[0m00:01\u001b[0m\n\u001b[?25hDownloading nvidia_cufft_cu12-11.2.1.3-py3-none-manylinux2014_x86_64.whl (211.5 MB)\n\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m211.5/211.5 MB\u001b[0m \u001b[31m75.2 MB/s\u001b[0m  \u001b[33m0:00:02\u001b[0mm0:00:01\u001b[0m00:01\u001b[0m\n\u001b[?25hDownloading nvidia_curand_cu12-10.3.5.147-py3-none-manylinux2014_x86_64.whl (56.3 MB)\n\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m56.3/56.3 MB\u001b[0m \u001b[31m48.8 MB/s\u001b[0m  \u001b[33m0:00:01\u001b[0m6m0:00:01\u001b[0m00:01\u001b[0m\n\u001b[?25hDownloading nvidia_cusolver_cu12-11.6.1.9-py3-none-manylinux2014_x86_64.whl (127.9 MB)\n\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m127.9/127.9 MB\u001b[0m \u001b[31m79.6 MB/s\u001b[0m  \u001b[33m0:00:01\u001b[0mm0:00:01\u001b[0m00:01\u001b[0m\n\u001b[?25hDownloading nvidia_cusparse_cu12-12.3.1.170-py3-none-manylinux2014_x86_64.whl (207.5 MB)\n\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m207.5/207.5 MB\u001b[0m \u001b[31m77.7 MB/s\u001b[0m  \u001b[33m0:00:02\u001b[0mm0:00:01\u001b[0m00:01\u001b[0m\n\u001b[?25hDownloading nvidia_nvjitlink_cu12-12.4.127-py3-none-manylinux2014_x86_64.whl (21.1 MB)\n\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m21.1/21.1 MB\u001b[0m \u001b[31m11.2 MB/s\u001b[0m  \u001b[33m0:00:01\u001b[0m6m0:00:01\u001b[0m\n\u001b[?25hInstalling collected packages: pyarrow, nvidia-nvjitlink-cu12, nvidia-curand-cu12, nvidia-cufft-cu12, nvidia-cuda-runtime-cu12, nvidia-cuda-nvrtc-cu12, nvidia-cuda-cupti-cu12, nvidia-cublas-cu12, nvidia-cusparse-cu12, nvidia-cudnn-cu12, huggingface-hub, nvidia-cusolver-cu12\n\u001b[2K  Attempting uninstall: pyarrow\n\u001b[2K    Found existing installation: pyarrow 19.0.0\n\u001b[2K    Uninstalling pyarrow-19.0.0:\n\u001b[2K      Successfully uninstalled pyarrow-19.0.0\n\u001b[2K  Attempting uninstall: nvidia-nvjitlink-cu12━━━\u001b[0m \u001b[32m 0/12\u001b[0m [pyarrow]\n\u001b[2K    Found existing installation: nvidia-nvjitlink-cu12 12.5.82m [pyarrow]\n\u001b[2K    Uninstalling nvidia-nvjitlink-cu12-12.5.82:m \u001b[32m 0/12\u001b[0m [pyarrow]\n\u001b[2K      Successfully uninstalled nvidia-nvjitlink-cu12-12.5.82━━━━━━\u001b[0m \u001b[32m 1/12\u001b[0m [nvidia-nvjitlink-cu12]\n\u001b[2K  Attempting uninstall: nvidia-curand-cu12━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m 1/12\u001b[0m [nvidia-nvjitlink-cu12]\n\u001b[2K    Found existing installation: nvidia-curand-cu12 10.3.6.82━\u001b[0m \u001b[32m 1/12\u001b[0m [nvidia-nvjitlink-cu12]\n\u001b[2K    Uninstalling nvidia-curand-cu12-10.3.6.82:━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m 1/12\u001b[0m [nvidia-nvjitlink-cu12]\n\u001b[2K      Successfully uninstalled nvidia-curand-cu12-10.3.6.82━━━━━━━\u001b[0m \u001b[32m 2/12\u001b[0m [nvidia-curand-cu12]\n\u001b[2K  Attempting uninstall: nvidia-cufft-cu12━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m 2/12\u001b[0m [nvidia-curand-cu12]\n\u001b[2K    Found existing installation: nvidia-cufft-cu12 11.2.3.61━━\u001b[0m \u001b[32m 2/12\u001b[0m [nvidia-curand-cu12]\n\u001b[2K    Uninstalling nvidia-cufft-cu12-11.2.3.61:━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m 2/12\u001b[0m [nvidia-curand-cu12]\n\u001b[2K      Successfully uninstalled nvidia-cufft-cu12-11.2.3.61━━━━━━━━\u001b[0m \u001b[32m 3/12\u001b[0m [nvidia-cufft-cu12]\n\u001b[2K  Attempting uninstall: nvidia-cuda-runtime-cu12━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m 3/12\u001b[0m [nvidia-cufft-cu12]\n\u001b[2K    Found existing installation: nvidia-cuda-runtime-cu12 12.5.82m \u001b[32m 3/12\u001b[0m [nvidia-cufft-cu12]\n\u001b[2K    Uninstalling nvidia-cuda-runtime-cu12-12.5.82:━━━━━━━━━━━━\u001b[0m \u001b[32m 3/12\u001b[0m [nvidia-cufft-cu12]\n\u001b[2K      Successfully uninstalled nvidia-cuda-runtime-cu12-12.5.82[0m \u001b[32m 3/12\u001b[0m [nvidia-cufft-cu12]\n\u001b[2K  Attempting uninstall: nvidia-cuda-nvrtc-cu12━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m 4/12\u001b[0m [nvidia-cuda-runtime-cu12]\n\u001b[2K    Found existing installation: nvidia-cuda-nvrtc-cu12 12.5.82[0m \u001b[32m 4/12\u001b[0m [nvidia-cuda-runtime-cu12]\n\u001b[2K    Uninstalling nvidia-cuda-nvrtc-cu12-12.5.82:━━━━━━━━━━━━━━\u001b[0m \u001b[32m 4/12\u001b[0m [nvidia-cuda-runtime-cu12]\n\u001b[2K      Successfully uninstalled nvidia-cuda-nvrtc-cu12-12.5.82━━━━━\u001b[0m \u001b[32m 5/12\u001b[0m [nvidia-cuda-nvrtc-cu12]\n\u001b[2K  Attempting uninstall: nvidia-cuda-cupti-cu12━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m 5/12\u001b[0m [nvidia-cuda-nvrtc-cu12]\n\u001b[2K    Found existing installation: nvidia-cuda-cupti-cu12 12.5.82[0m \u001b[32m 5/12\u001b[0m [nvidia-cuda-nvrtc-cu12]\n\u001b[2K    Uninstalling nvidia-cuda-cupti-cu12-12.5.82:━━━━━━━━━━━━━━\u001b[0m \u001b[32m 5/12\u001b[0m [nvidia-cuda-nvrtc-cu12]\n\u001b[2K      Successfully uninstalled nvidia-cuda-cupti-cu12-12.5.82━━━━━\u001b[0m \u001b[32m 6/12\u001b[0m [nvidia-cuda-cupti-cu12]\n\u001b[2K  Attempting uninstall: nvidia-cublas-cu12\u001b[90m━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m 6/12\u001b[0m [nvidia-cuda-cupti-cu12]\n\u001b[2K    Found existing installation: nvidia-cublas-cu12 12.5.3.2━━\u001b[0m \u001b[32m 6/12\u001b[0m [nvidia-cuda-cupti-cu12]\n\u001b[2K    Uninstalling nvidia-cublas-cu12-12.5.3.2:━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m 6/12\u001b[0m [nvidia-cuda-cupti-cu12]\n\u001b[2K      Successfully uninstalled nvidia-cublas-cu12-12.5.3.2━━━━━━━━\u001b[0m \u001b[32m 7/12\u001b[0m [nvidia-cublas-cu12]\n\u001b[2K  Attempting uninstall: nvidia-cusparse-cu12m\u001b[90m━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m 7/12\u001b[0m [nvidia-cublas-cu12]\n\u001b[2K    Found existing installation: nvidia-cusparse-cu12 12.5.1.3\u001b[0m \u001b[32m 7/12\u001b[0m [nvidia-cublas-cu12]\n\u001b[2K    Uninstalling nvidia-cusparse-cu12-12.5.1.3:━━━━━━━━━━━━━━━\u001b[0m \u001b[32m 7/12\u001b[0m [nvidia-cublas-cu12]\n\u001b[2K      Successfully uninstalled nvidia-cusparse-cu12-12.5.1.3━━━━━━\u001b[0m \u001b[32m 8/12\u001b[0m [nvidia-cusparse-cu12]\n\u001b[2K  Attempting uninstall: nvidia-cudnn-cu121m╸\u001b[0m\u001b[90m━━━━━━━━━━━━━\u001b[0m \u001b[32m 8/12\u001b[0m [nvidia-cusparse-cu12]\n\u001b[2K    Found existing installation: nvidia-cudnn-cu12 9.3.0.75━━━\u001b[0m \u001b[32m 8/12\u001b[0m [nvidia-cusparse-cu12]\n\u001b[2K    Uninstalling nvidia-cudnn-cu12-9.3.0.75:\u001b[90m━━━━━━━━━━━━━\u001b[0m \u001b[32m 8/12\u001b[0m [nvidia-cusparse-cu12]\n\u001b[2K      Successfully uninstalled nvidia-cudnn-cu12-9.3.0.75━━━━━━━━━\u001b[0m \u001b[32m 9/12\u001b[0m [nvidia-cudnn-cu12]\n\u001b[2K  Attempting uninstall: huggingface-hub[0m\u001b[90m╺\u001b[0m\u001b[90m━━━━━━━━━\u001b[0m \u001b[32m 9/12\u001b[0m [nvidia-cudnn-cu12]\n\u001b[2K    Found existing installation: huggingface-hub 1.0.0rc2━━━━━\u001b[0m \u001b[32m 9/12\u001b[0m [nvidia-cudnn-cu12]\n\u001b[2K    Uninstalling huggingface-hub-1.0.0rc2:m╺\u001b[0m\u001b[90m━━━━━━━━━\u001b[0m \u001b[32m 9/12\u001b[0m [nvidia-cudnn-cu12]\n\u001b[2K      Successfully uninstalled huggingface-hub-1.0.0rc2\u001b[90m━━━━━━\u001b[0m \u001b[32m10/12\u001b[0m [huggingface-hub]\n\u001b[2K  Attempting uninstall: nvidia-cusolver-cu12m\u001b[90m╺\u001b[0m\u001b[90m━━━━━━\u001b[0m \u001b[32m10/12\u001b[0m [huggingface-hub]\n\u001b[2K    Found existing installation: nvidia-cusolver-cu12 11.6.3.83[0m \u001b[32m10/12\u001b[0m [huggingface-hub]\n\u001b[2K    Uninstalling nvidia-cusolver-cu12-11.6.3.83:[0m\u001b[90m━━━━━━\u001b[0m \u001b[32m10/12\u001b[0m [huggingface-hub]\n\u001b[2K      Successfully uninstalled nvidia-cusolver-cu12-11.6.3.830m━━━\u001b[0m \u001b[32m11/12\u001b[0m [nvidia-cusolver-cu12]\n\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m12/12\u001b[0m [nvidia-cusolver-cu12]dia-cusolver-cu12]\n\u001b[1A\u001b[2K\u001b[31mERROR: pip's dependency resolver does not currently take into account all the packages that are installed. This behaviour is the source of the following dependency conflicts.\nbigframes 2.12.0 requires google-cloud-bigquery-storage<3.0.0,>=2.30.0, which is not installed.\npylibcudf-cu12 25.2.2 requires pyarrow<20.0.0a0,>=14.0.0; platform_machine == \"x86_64\", but you have pyarrow 22.0.0 which is incompatible.\ncudf-cu12 25.2.2 requires pyarrow<20.0.0a0,>=14.0.0; platform_machine == \"x86_64\", but you have pyarrow 22.0.0 which is incompatible.\nbigframes 2.12.0 requires google-cloud-bigquery[bqstorage,pandas]>=3.31.0, but you have google-cloud-bigquery 3.25.0 which is incompatible.\nlibcugraph-cu12 25.6.0 requires libraft-cu12==25.6.*, but you have libraft-cu12 25.2.0 which is incompatible.\ncudf-polars-cu12 25.6.0 requires pylibcudf-cu12==25.6.*, but you have pylibcudf-cu12 25.2.2 which is incompatible.\npandas-gbq 0.29.2 requires google-api-core<3.0.0,>=2.10.2, but you have google-api-core 1.34.1 which is incompatible.\npylibcugraph-cu12 25.6.0 requires pylibraft-cu12==25.6.*, but you have pylibraft-cu12 25.2.0 which is incompatible.\npylibcugraph-cu12 25.6.0 requires rmm-cu12==25.6.*, but you have rmm-cu12 25.2.0 which is incompatible.\u001b[0m\u001b[31m\n\u001b[0mSuccessfully installed huggingface-hub-0.36.0 nvidia-cublas-cu12-12.4.5.8 nvidia-cuda-cupti-cu12-12.4.127 nvidia-cuda-nvrtc-cu12-12.4.127 nvidia-cuda-runtime-cu12-12.4.127 nvidia-cudnn-cu12-9.1.0.70 nvidia-cufft-cu12-11.2.1.3 nvidia-curand-cu12-10.3.5.147 nvidia-cusolver-cu12-11.6.1.9 nvidia-cusparse-cu12-12.3.1.170 nvidia-nvjitlink-cu12-12.4.127 pyarrow-22.0.0\n","output_type":"stream"}],"execution_count":5},{"cell_type":"code","source":"!pip install audiomentations==0.36.0","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-11-06T16:23:12.013252Z","iopub.execute_input":"2025-11-06T16:23:12.013558Z","iopub.status.idle":"2025-11-06T16:23:21.061179Z","shell.execute_reply.started":"2025-11-06T16:23:12.013534Z","shell.execute_reply":"2025-11-06T16:23:21.060447Z"}},"outputs":[{"name":"stdout","text":"Collecting audiomentations==0.36.0\n  Downloading audiomentations-0.36.0-py3-none-any.whl.metadata (10 kB)\nRequirement already satisfied: numpy>=1.21.0 in /usr/local/lib/python3.11/dist-packages (from audiomentations==0.36.0) (1.26.4)\nCollecting librosa!=0.10.0,<0.11.0,>=0.8.0 (from audiomentations==0.36.0)\n  Downloading librosa-0.10.2.post1-py3-none-any.whl.metadata (8.6 kB)\nCollecting scipy<1.13,>=1.4 (from audiomentations==0.36.0)\n  Downloading scipy-1.12.0-cp311-cp311-manylinux_2_17_x86_64.manylinux2014_x86_64.whl.metadata (60 kB)\nRequirement already satisfied: soxr<1.0.0,>=0.3.2 in /usr/local/lib/python3.11/dist-packages (from audiomentations==0.36.0) (0.5.0.post1)\nRequirement already satisfied: audioread>=2.1.9 in /usr/local/lib/python3.11/dist-packages (from librosa!=0.10.0,<0.11.0,>=0.8.0->audiomentations==0.36.0) (3.0.1)\nRequirement already satisfied: scikit-learn>=0.20.0 in /usr/local/lib/python3.11/dist-packages (from librosa!=0.10.0,<0.11.0,>=0.8.0->audiomentations==0.36.0) (1.2.2)\nRequirement already satisfied: joblib>=0.14 in /usr/local/lib/python3.11/dist-packages (from librosa!=0.10.0,<0.11.0,>=0.8.0->audiomentations==0.36.0) (1.5.2)\nRequirement already satisfied: decorator>=4.3.0 in /usr/local/lib/python3.11/dist-packages (from librosa!=0.10.0,<0.11.0,>=0.8.0->audiomentations==0.36.0) (4.4.2)\nRequirement already satisfied: numba>=0.51.0 in /usr/local/lib/python3.11/dist-packages (from librosa!=0.10.0,<0.11.0,>=0.8.0->audiomentations==0.36.0) (0.60.0)\nRequirement already satisfied: soundfile>=0.12.1 in /usr/local/lib/python3.11/dist-packages (from librosa!=0.10.0,<0.11.0,>=0.8.0->audiomentations==0.36.0) (0.13.1)\nRequirement already satisfied: pooch>=1.1 in /usr/local/lib/python3.11/dist-packages (from librosa!=0.10.0,<0.11.0,>=0.8.0->audiomentations==0.36.0) (1.8.2)\nRequirement already satisfied: typing-extensions>=4.1.1 in /usr/local/lib/python3.11/dist-packages (from librosa!=0.10.0,<0.11.0,>=0.8.0->audiomentations==0.36.0) (4.15.0)\nRequirement already satisfied: lazy-loader>=0.1 in /usr/local/lib/python3.11/dist-packages (from librosa!=0.10.0,<0.11.0,>=0.8.0->audiomentations==0.36.0) (0.4)\nRequirement already satisfied: msgpack>=1.0 in /usr/local/lib/python3.11/dist-packages (from librosa!=0.10.0,<0.11.0,>=0.8.0->audiomentations==0.36.0) (1.1.1)\nRequirement already satisfied: mkl_fft in /usr/local/lib/python3.11/dist-packages (from numpy>=1.21.0->audiomentations==0.36.0) (1.3.8)\nRequirement already satisfied: mkl_random in /usr/local/lib/python3.11/dist-packages (from numpy>=1.21.0->audiomentations==0.36.0) (1.2.4)\nRequirement already satisfied: mkl_umath in /usr/local/lib/python3.11/dist-packages (from numpy>=1.21.0->audiomentations==0.36.0) (0.1.1)\nRequirement already satisfied: mkl in /usr/local/lib/python3.11/dist-packages (from numpy>=1.21.0->audiomentations==0.36.0) (2025.2.0)\nRequirement already satisfied: tbb4py in /usr/local/lib/python3.11/dist-packages (from numpy>=1.21.0->audiomentations==0.36.0) (2022.2.0)\nRequirement already satisfied: mkl-service in /usr/local/lib/python3.11/dist-packages (from numpy>=1.21.0->audiomentations==0.36.0) (2.4.1)\nRequirement already satisfied: packaging in /usr/local/lib/python3.11/dist-packages (from lazy-loader>=0.1->librosa!=0.10.0,<0.11.0,>=0.8.0->audiomentations==0.36.0) (25.0)\nRequirement already satisfied: llvmlite<0.44,>=0.43.0dev0 in /usr/local/lib/python3.11/dist-packages (from numba>=0.51.0->librosa!=0.10.0,<0.11.0,>=0.8.0->audiomentations==0.36.0) (0.43.0)\nRequirement already satisfied: platformdirs>=2.5.0 in /usr/local/lib/python3.11/dist-packages (from pooch>=1.1->librosa!=0.10.0,<0.11.0,>=0.8.0->audiomentations==0.36.0) (4.4.0)\nRequirement already satisfied: requests>=2.19.0 in /usr/local/lib/python3.11/dist-packages (from pooch>=1.1->librosa!=0.10.0,<0.11.0,>=0.8.0->audiomentations==0.36.0) (2.32.5)\nRequirement already satisfied: charset_normalizer<4,>=2 in /usr/local/lib/python3.11/dist-packages (from requests>=2.19.0->pooch>=1.1->librosa!=0.10.0,<0.11.0,>=0.8.0->audiomentations==0.36.0) (3.4.3)\nRequirement already satisfied: idna<4,>=2.5 in /usr/local/lib/python3.11/dist-packages (from requests>=2.19.0->pooch>=1.1->librosa!=0.10.0,<0.11.0,>=0.8.0->audiomentations==0.36.0) (3.10)\nRequirement already satisfied: urllib3<3,>=1.21.1 in /usr/local/lib/python3.11/dist-packages (from requests>=2.19.0->pooch>=1.1->librosa!=0.10.0,<0.11.0,>=0.8.0->audiomentations==0.36.0) (2.5.0)\nRequirement already satisfied: certifi>=2017.4.17 in /usr/local/lib/python3.11/dist-packages (from requests>=2.19.0->pooch>=1.1->librosa!=0.10.0,<0.11.0,>=0.8.0->audiomentations==0.36.0) (2025.8.3)\nRequirement already satisfied: threadpoolctl>=2.0.0 in /usr/local/lib/python3.11/dist-packages (from scikit-learn>=0.20.0->librosa!=0.10.0,<0.11.0,>=0.8.0->audiomentations==0.36.0) (3.6.0)\nRequirement already satisfied: cffi>=1.0 in /usr/local/lib/python3.11/dist-packages (from soundfile>=0.12.1->librosa!=0.10.0,<0.11.0,>=0.8.0->audiomentations==0.36.0) (2.0.0)\nRequirement already satisfied: pycparser in /usr/local/lib/python3.11/dist-packages (from cffi>=1.0->soundfile>=0.12.1->librosa!=0.10.0,<0.11.0,>=0.8.0->audiomentations==0.36.0) (2.23)\nRequirement already satisfied: intel-openmp<2026,>=2024 in /usr/local/lib/python3.11/dist-packages (from mkl->numpy>=1.21.0->audiomentations==0.36.0) (2024.2.0)\nRequirement already satisfied: tbb==2022.* in /usr/local/lib/python3.11/dist-packages (from mkl->numpy>=1.21.0->audiomentations==0.36.0) (2022.2.0)\nRequirement already satisfied: intel-cmplr-lib-ur==2024.2.0 in /usr/local/lib/python3.11/dist-packages (from intel-openmp<2026,>=2024->mkl->numpy>=1.21.0->audiomentations==0.36.0) (2024.2.0)\nRequirement already satisfied: tcmlib==1.* in /usr/local/lib/python3.11/dist-packages (from tbb==2022.*->mkl->numpy>=1.21.0->audiomentations==0.36.0) (1.4.0)\nRequirement already satisfied: intel-cmplr-lib-rt in /usr/local/lib/python3.11/dist-packages (from mkl_umath->numpy>=1.21.0->audiomentations==0.36.0) (2024.2.0)\nDownloading audiomentations-0.36.0-py3-none-any.whl (80 kB)\nDownloading librosa-0.10.2.post1-py3-none-any.whl (260 kB)\nDownloading scipy-1.12.0-cp311-cp311-manylinux_2_17_x86_64.manylinux2014_x86_64.whl (38.4 MB)\n\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m38.4/38.4 MB\u001b[0m \u001b[31m95.5 MB/s\u001b[0m  \u001b[33m0:00:00\u001b[0m6m0:00:01\u001b[0m00:01\u001b[0m\n\u001b[?25hInstalling collected packages: scipy, librosa, audiomentations\n\u001b[2K  Attempting uninstall: scipy\n\u001b[2K    Found existing installation: scipy 1.15.3\n\u001b[2K    Uninstalling scipy-1.15.3:\n\u001b[2K      Successfully uninstalled scipy-1.15.3━━━━━\u001b[0m \u001b[32m0/3\u001b[0m [scipy]\n\u001b[2K  Attempting uninstall: librosa━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m0/3\u001b[0m [scipy]\n\u001b[2K    Found existing installation: librosa 0.11.0m \u001b[32m0/3\u001b[0m [scipy]\n\u001b[2K    Uninstalling librosa-0.11.0:━━━━━━━━━━━━\u001b[0m \u001b[32m0/3\u001b[0m [scipy]\n\u001b[2K      Successfully uninstalled librosa-0.11.0━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m1/3\u001b[0m [librosa]\n\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m3/3\u001b[0m [audiomentations] [audiomentations]\n\u001b[1A\u001b[2K\u001b[31mERROR: pip's dependency resolver does not currently take into account all the packages that are installed. This behaviour is the source of the following dependency conflicts.\ncesium 0.12.4 requires numpy<3.0,>=2.0, but you have numpy 1.26.4 which is incompatible.\ntsfresh 0.21.0 requires scipy>=1.14.0; python_version >= \"3.10\", but you have scipy 1.12.0 which is incompatible.\ndopamine-rl 4.1.2 requires gymnasium>=1.0.0, but you have gymnasium 0.29.0 which is incompatible.\nimbalanced-learn 0.13.0 requires scikit-learn<2,>=1.3.2, but you have scikit-learn 1.2.2 which is incompatible.\nplotnine 0.14.5 requires matplotlib>=3.8.0, but you have matplotlib 3.7.2 which is incompatible.\numap-learn 0.5.9.post2 requires scikit-learn>=1.6, but you have scikit-learn 1.2.2 which is incompatible.\nmlxtend 0.23.4 requires scikit-learn>=1.3.1, but you have scikit-learn 1.2.2 which is incompatible.\u001b[0m\u001b[31m\n\u001b[0mSuccessfully installed audiomentations-0.36.0 librosa-0.10.2.post1 scipy-1.12.0\n","output_type":"stream"}],"execution_count":6},{"cell_type":"code","source":"!pip install  tensorboard numpy scipy datasets ","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-11-06T16:23:21.062308Z","iopub.execute_input":"2025-11-06T16:23:21.062615Z","iopub.status.idle":"2025-11-06T16:23:23.078682Z","shell.execute_reply.started":"2025-11-06T16:23:21.062579Z","shell.execute_reply":"2025-11-06T16:23:23.077746Z"}},"outputs":[{"name":"stdout","text":"Requirement already satisfied: tensorboard in /usr/local/lib/python3.11/dist-packages (2.18.0)\nRequirement already satisfied: numpy in /usr/local/lib/python3.11/dist-packages (1.26.4)\nRequirement already satisfied: scipy in /usr/local/lib/python3.11/dist-packages (1.12.0)\nRequirement already satisfied: datasets in /usr/local/lib/python3.11/dist-packages (4.1.1)\nRequirement already satisfied: absl-py>=0.4 in /usr/local/lib/python3.11/dist-packages (from tensorboard) (1.4.0)\nRequirement already satisfied: grpcio>=1.48.2 in /usr/local/lib/python3.11/dist-packages (from tensorboard) (1.75.1)\nRequirement already satisfied: markdown>=2.6.8 in /usr/local/lib/python3.11/dist-packages (from tensorboard) (3.8.2)\nRequirement already satisfied: packaging in /usr/local/lib/python3.11/dist-packages (from tensorboard) (25.0)\nRequirement already satisfied: protobuf!=4.24.0,>=3.19.6 in /usr/local/lib/python3.11/dist-packages (from tensorboard) (3.20.3)\nRequirement already satisfied: setuptools>=41.0.0 in /usr/local/lib/python3.11/dist-packages (from tensorboard) (75.2.0)\nRequirement already satisfied: six>1.9 in /usr/local/lib/python3.11/dist-packages (from tensorboard) (1.17.0)\nRequirement already satisfied: tensorboard-data-server<0.8.0,>=0.7.0 in /usr/local/lib/python3.11/dist-packages (from tensorboard) (0.7.2)\nRequirement already satisfied: werkzeug>=1.0.1 in /usr/local/lib/python3.11/dist-packages (from tensorboard) (3.1.3)\nRequirement already satisfied: mkl_fft in /usr/local/lib/python3.11/dist-packages (from numpy) (1.3.8)\nRequirement already satisfied: mkl_random in /usr/local/lib/python3.11/dist-packages (from numpy) (1.2.4)\nRequirement already satisfied: mkl_umath in /usr/local/lib/python3.11/dist-packages (from numpy) (0.1.1)\nRequirement already satisfied: mkl in /usr/local/lib/python3.11/dist-packages (from numpy) (2025.2.0)\nRequirement already satisfied: tbb4py in /usr/local/lib/python3.11/dist-packages (from numpy) (2022.2.0)\nRequirement already satisfied: mkl-service in /usr/local/lib/python3.11/dist-packages (from numpy) (2.4.1)\nRequirement already satisfied: filelock in /usr/local/lib/python3.11/dist-packages (from datasets) (3.19.1)\nRequirement already satisfied: pyarrow>=21.0.0 in /usr/local/lib/python3.11/dist-packages (from datasets) (22.0.0)\nRequirement already satisfied: dill<0.4.1,>=0.3.0 in /usr/local/lib/python3.11/dist-packages (from datasets) (0.4.0)\nRequirement already satisfied: pandas in /usr/local/lib/python3.11/dist-packages (from datasets) (2.2.3)\nRequirement already satisfied: requests>=2.32.2 in /usr/local/lib/python3.11/dist-packages (from datasets) (2.32.5)\nRequirement already satisfied: tqdm>=4.66.3 in /usr/local/lib/python3.11/dist-packages (from datasets) (4.67.1)\nRequirement already satisfied: xxhash in /usr/local/lib/python3.11/dist-packages (from datasets) (3.5.0)\nRequirement already satisfied: multiprocess<0.70.17 in /usr/local/lib/python3.11/dist-packages (from datasets) (0.70.16)\nRequirement already satisfied: fsspec<=2025.9.0,>=2023.1.0 in /usr/local/lib/python3.11/dist-packages (from fsspec[http]<=2025.9.0,>=2023.1.0->datasets) (2025.9.0)\nRequirement already satisfied: huggingface-hub>=0.24.0 in /usr/local/lib/python3.11/dist-packages (from datasets) (0.36.0)\nRequirement already satisfied: pyyaml>=5.1 in /usr/local/lib/python3.11/dist-packages (from datasets) (6.0.3)\nRequirement already satisfied: aiohttp!=4.0.0a0,!=4.0.0a1 in /usr/local/lib/python3.11/dist-packages (from fsspec[http]<=2025.9.0,>=2023.1.0->datasets) (3.12.15)\nRequirement already satisfied: aiohappyeyeballs>=2.5.0 in /usr/local/lib/python3.11/dist-packages (from aiohttp!=4.0.0a0,!=4.0.0a1->fsspec[http]<=2025.9.0,>=2023.1.0->datasets) (2.6.1)\nRequirement already satisfied: aiosignal>=1.4.0 in /usr/local/lib/python3.11/dist-packages (from aiohttp!=4.0.0a0,!=4.0.0a1->fsspec[http]<=2025.9.0,>=2023.1.0->datasets) (1.4.0)\nRequirement already satisfied: attrs>=17.3.0 in /usr/local/lib/python3.11/dist-packages (from aiohttp!=4.0.0a0,!=4.0.0a1->fsspec[http]<=2025.9.0,>=2023.1.0->datasets) (25.3.0)\nRequirement already satisfied: frozenlist>=1.1.1 in /usr/local/lib/python3.11/dist-packages (from aiohttp!=4.0.0a0,!=4.0.0a1->fsspec[http]<=2025.9.0,>=2023.1.0->datasets) (1.7.0)\nRequirement already satisfied: multidict<7.0,>=4.5 in /usr/local/lib/python3.11/dist-packages (from aiohttp!=4.0.0a0,!=4.0.0a1->fsspec[http]<=2025.9.0,>=2023.1.0->datasets) (6.6.4)\nRequirement already satisfied: propcache>=0.2.0 in /usr/local/lib/python3.11/dist-packages (from aiohttp!=4.0.0a0,!=4.0.0a1->fsspec[http]<=2025.9.0,>=2023.1.0->datasets) (0.3.2)\nRequirement already satisfied: yarl<2.0,>=1.17.0 in /usr/local/lib/python3.11/dist-packages (from aiohttp!=4.0.0a0,!=4.0.0a1->fsspec[http]<=2025.9.0,>=2023.1.0->datasets) (1.20.1)\nRequirement already satisfied: idna>=2.0 in /usr/local/lib/python3.11/dist-packages (from yarl<2.0,>=1.17.0->aiohttp!=4.0.0a0,!=4.0.0a1->fsspec[http]<=2025.9.0,>=2023.1.0->datasets) (3.10)\nRequirement already satisfied: typing-extensions>=4.2 in /usr/local/lib/python3.11/dist-packages (from aiosignal>=1.4.0->aiohttp!=4.0.0a0,!=4.0.0a1->fsspec[http]<=2025.9.0,>=2023.1.0->datasets) (4.15.0)\nRequirement already satisfied: hf-xet<2.0.0,>=1.1.3 in /usr/local/lib/python3.11/dist-packages (from huggingface-hub>=0.24.0->datasets) (1.1.10)\nRequirement already satisfied: charset_normalizer<4,>=2 in /usr/local/lib/python3.11/dist-packages (from requests>=2.32.2->datasets) (3.4.3)\nRequirement already satisfied: urllib3<3,>=1.21.1 in /usr/local/lib/python3.11/dist-packages (from requests>=2.32.2->datasets) (2.5.0)\nRequirement already satisfied: certifi>=2017.4.17 in /usr/local/lib/python3.11/dist-packages (from requests>=2.32.2->datasets) (2025.8.3)\nRequirement already satisfied: MarkupSafe>=2.1.1 in /usr/local/lib/python3.11/dist-packages (from werkzeug>=1.0.1->tensorboard) (3.0.2)\nRequirement already satisfied: intel-openmp<2026,>=2024 in /usr/local/lib/python3.11/dist-packages (from mkl->numpy) (2024.2.0)\nRequirement already satisfied: tbb==2022.* in /usr/local/lib/python3.11/dist-packages (from mkl->numpy) (2022.2.0)\nRequirement already satisfied: intel-cmplr-lib-ur==2024.2.0 in /usr/local/lib/python3.11/dist-packages (from intel-openmp<2026,>=2024->mkl->numpy) (2024.2.0)\nRequirement already satisfied: tcmlib==1.* in /usr/local/lib/python3.11/dist-packages (from tbb==2022.*->mkl->numpy) (1.4.0)\nRequirement already satisfied: intel-cmplr-lib-rt in /usr/local/lib/python3.11/dist-packages (from mkl_umath->numpy) (2024.2.0)\nRequirement already satisfied: python-dateutil>=2.8.2 in /usr/local/lib/python3.11/dist-packages (from pandas->datasets) (2.9.0.post0)\nRequirement already satisfied: pytz>=2020.1 in /usr/local/lib/python3.11/dist-packages (from pandas->datasets) (2025.2)\nRequirement already satisfied: tzdata>=2022.7 in /usr/local/lib/python3.11/dist-packages (from pandas->datasets) (2025.2)\n","output_type":"stream"}],"execution_count":7},{"cell_type":"code","source":"!pip install peft>=0.12.0 bitsandbytes>=0.43.3","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-11-06T16:23:23.079683Z","iopub.execute_input":"2025-11-06T16:23:23.079968Z","iopub.status.idle":"2025-11-06T16:23:27.505578Z","shell.execute_reply.started":"2025-11-06T16:23:23.079923Z","shell.execute_reply":"2025-11-06T16:23:27.504447Z"}},"outputs":[],"execution_count":8},{"cell_type":"code","source":"!pip install tqdm==4.67.1 scikit-learn==1.2.2","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-11-06T16:23:27.506902Z","iopub.execute_input":"2025-11-06T16:23:27.507231Z","iopub.status.idle":"2025-11-06T16:23:29.481500Z","shell.execute_reply.started":"2025-11-06T16:23:27.507198Z","shell.execute_reply":"2025-11-06T16:23:29.480535Z"}},"outputs":[{"name":"stdout","text":"Requirement already satisfied: tqdm==4.67.1 in /usr/local/lib/python3.11/dist-packages (4.67.1)\nRequirement already satisfied: scikit-learn==1.2.2 in /usr/local/lib/python3.11/dist-packages (1.2.2)\nRequirement already satisfied: numpy>=1.17.3 in /usr/local/lib/python3.11/dist-packages (from scikit-learn==1.2.2) (1.26.4)\nRequirement already satisfied: scipy>=1.3.2 in /usr/local/lib/python3.11/dist-packages (from scikit-learn==1.2.2) (1.12.0)\nRequirement already satisfied: joblib>=1.1.1 in /usr/local/lib/python3.11/dist-packages (from scikit-learn==1.2.2) (1.5.2)\nRequirement already satisfied: threadpoolctl>=2.0.0 in /usr/local/lib/python3.11/dist-packages (from scikit-learn==1.2.2) (3.6.0)\nRequirement already satisfied: mkl_fft in /usr/local/lib/python3.11/dist-packages (from numpy>=1.17.3->scikit-learn==1.2.2) (1.3.8)\nRequirement already satisfied: mkl_random in /usr/local/lib/python3.11/dist-packages (from numpy>=1.17.3->scikit-learn==1.2.2) (1.2.4)\nRequirement already satisfied: mkl_umath in /usr/local/lib/python3.11/dist-packages (from numpy>=1.17.3->scikit-learn==1.2.2) (0.1.1)\nRequirement already satisfied: mkl in /usr/local/lib/python3.11/dist-packages (from numpy>=1.17.3->scikit-learn==1.2.2) (2025.2.0)\nRequirement already satisfied: tbb4py in /usr/local/lib/python3.11/dist-packages (from numpy>=1.17.3->scikit-learn==1.2.2) (2022.2.0)\nRequirement already satisfied: mkl-service in /usr/local/lib/python3.11/dist-packages (from numpy>=1.17.3->scikit-learn==1.2.2) (2.4.1)\nRequirement already satisfied: intel-openmp<2026,>=2024 in /usr/local/lib/python3.11/dist-packages (from mkl->numpy>=1.17.3->scikit-learn==1.2.2) (2024.2.0)\nRequirement already satisfied: tbb==2022.* in /usr/local/lib/python3.11/dist-packages (from mkl->numpy>=1.17.3->scikit-learn==1.2.2) (2022.2.0)\nRequirement already satisfied: intel-cmplr-lib-ur==2024.2.0 in /usr/local/lib/python3.11/dist-packages (from intel-openmp<2026,>=2024->mkl->numpy>=1.17.3->scikit-learn==1.2.2) (2024.2.0)\nRequirement already satisfied: tcmlib==1.* in /usr/local/lib/python3.11/dist-packages (from tbb==2022.*->mkl->numpy>=1.17.3->scikit-learn==1.2.2) (1.4.0)\nRequirement already satisfied: intel-cmplr-lib-rt in /usr/local/lib/python3.11/dist-packages (from mkl_umath->numpy>=1.17.3->scikit-learn==1.2.2) (2024.2.0)\n","output_type":"stream"}],"execution_count":9},{"cell_type":"code","source":"pip install evaluate jiwer","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-11-06T16:23:29.482610Z","iopub.execute_input":"2025-11-06T16:23:29.482863Z","iopub.status.idle":"2025-11-06T16:23:34.016759Z","shell.execute_reply.started":"2025-11-06T16:23:29.482841Z","shell.execute_reply":"2025-11-06T16:23:34.015951Z"}},"outputs":[{"name":"stdout","text":"Collecting evaluate\n  Downloading evaluate-0.4.6-py3-none-any.whl.metadata (9.5 kB)\nCollecting jiwer\n  Downloading jiwer-4.0.0-py3-none-any.whl.metadata (3.3 kB)\nRequirement already satisfied: datasets>=2.0.0 in /usr/local/lib/python3.11/dist-packages (from evaluate) (4.1.1)\nRequirement already satisfied: numpy>=1.17 in /usr/local/lib/python3.11/dist-packages (from evaluate) (1.26.4)\nRequirement already satisfied: dill in /usr/local/lib/python3.11/dist-packages (from evaluate) (0.4.0)\nRequirement already satisfied: pandas in /usr/local/lib/python3.11/dist-packages (from evaluate) (2.2.3)\nRequirement already satisfied: requests>=2.19.0 in /usr/local/lib/python3.11/dist-packages (from evaluate) (2.32.5)\nRequirement already satisfied: tqdm>=4.62.1 in /usr/local/lib/python3.11/dist-packages (from evaluate) (4.67.1)\nRequirement already satisfied: xxhash in /usr/local/lib/python3.11/dist-packages (from evaluate) (3.5.0)\nRequirement already satisfied: multiprocess in /usr/local/lib/python3.11/dist-packages (from evaluate) (0.70.16)\nRequirement already satisfied: fsspec>=2021.05.0 in /usr/local/lib/python3.11/dist-packages (from fsspec[http]>=2021.05.0->evaluate) (2025.9.0)\nRequirement already satisfied: huggingface-hub>=0.7.0 in /usr/local/lib/python3.11/dist-packages (from evaluate) (0.36.0)\nRequirement already satisfied: packaging in /usr/local/lib/python3.11/dist-packages (from evaluate) (25.0)\nRequirement already satisfied: click>=8.1.8 in /usr/local/lib/python3.11/dist-packages (from jiwer) (8.3.0)\nCollecting rapidfuzz>=3.9.7 (from jiwer)\n  Downloading rapidfuzz-3.14.3-cp311-cp311-manylinux_2_27_x86_64.manylinux_2_28_x86_64.whl.metadata (12 kB)\nRequirement already satisfied: filelock in /usr/local/lib/python3.11/dist-packages (from datasets>=2.0.0->evaluate) (3.19.1)\nRequirement already satisfied: pyarrow>=21.0.0 in /usr/local/lib/python3.11/dist-packages (from datasets>=2.0.0->evaluate) (22.0.0)\nRequirement already satisfied: pyyaml>=5.1 in /usr/local/lib/python3.11/dist-packages (from datasets>=2.0.0->evaluate) (6.0.3)\nRequirement already satisfied: aiohttp!=4.0.0a0,!=4.0.0a1 in /usr/local/lib/python3.11/dist-packages (from fsspec[http]>=2021.05.0->evaluate) (3.12.15)\nRequirement already satisfied: aiohappyeyeballs>=2.5.0 in /usr/local/lib/python3.11/dist-packages (from aiohttp!=4.0.0a0,!=4.0.0a1->fsspec[http]>=2021.05.0->evaluate) (2.6.1)\nRequirement already satisfied: aiosignal>=1.4.0 in /usr/local/lib/python3.11/dist-packages (from aiohttp!=4.0.0a0,!=4.0.0a1->fsspec[http]>=2021.05.0->evaluate) (1.4.0)\nRequirement already satisfied: attrs>=17.3.0 in /usr/local/lib/python3.11/dist-packages (from aiohttp!=4.0.0a0,!=4.0.0a1->fsspec[http]>=2021.05.0->evaluate) (25.3.0)\nRequirement already satisfied: frozenlist>=1.1.1 in /usr/local/lib/python3.11/dist-packages (from aiohttp!=4.0.0a0,!=4.0.0a1->fsspec[http]>=2021.05.0->evaluate) (1.7.0)\nRequirement already satisfied: multidict<7.0,>=4.5 in /usr/local/lib/python3.11/dist-packages (from aiohttp!=4.0.0a0,!=4.0.0a1->fsspec[http]>=2021.05.0->evaluate) (6.6.4)\nRequirement already satisfied: propcache>=0.2.0 in /usr/local/lib/python3.11/dist-packages (from aiohttp!=4.0.0a0,!=4.0.0a1->fsspec[http]>=2021.05.0->evaluate) (0.3.2)\nRequirement already satisfied: yarl<2.0,>=1.17.0 in /usr/local/lib/python3.11/dist-packages (from aiohttp!=4.0.0a0,!=4.0.0a1->fsspec[http]>=2021.05.0->evaluate) (1.20.1)\nRequirement already satisfied: idna>=2.0 in /usr/local/lib/python3.11/dist-packages (from yarl<2.0,>=1.17.0->aiohttp!=4.0.0a0,!=4.0.0a1->fsspec[http]>=2021.05.0->evaluate) (3.10)\nRequirement already satisfied: typing-extensions>=4.2 in /usr/local/lib/python3.11/dist-packages (from aiosignal>=1.4.0->aiohttp!=4.0.0a0,!=4.0.0a1->fsspec[http]>=2021.05.0->evaluate) (4.15.0)\nRequirement already satisfied: hf-xet<2.0.0,>=1.1.3 in /usr/local/lib/python3.11/dist-packages (from huggingface-hub>=0.7.0->evaluate) (1.1.10)\nRequirement already satisfied: mkl_fft in /usr/local/lib/python3.11/dist-packages (from numpy>=1.17->evaluate) (1.3.8)\nRequirement already satisfied: mkl_random in /usr/local/lib/python3.11/dist-packages (from numpy>=1.17->evaluate) (1.2.4)\nRequirement already satisfied: mkl_umath in /usr/local/lib/python3.11/dist-packages (from numpy>=1.17->evaluate) (0.1.1)\nRequirement already satisfied: mkl in /usr/local/lib/python3.11/dist-packages (from numpy>=1.17->evaluate) (2025.2.0)\nRequirement already satisfied: tbb4py in /usr/local/lib/python3.11/dist-packages (from numpy>=1.17->evaluate) (2022.2.0)\nRequirement already satisfied: mkl-service in /usr/local/lib/python3.11/dist-packages (from numpy>=1.17->evaluate) (2.4.1)\nRequirement already satisfied: charset_normalizer<4,>=2 in /usr/local/lib/python3.11/dist-packages (from requests>=2.19.0->evaluate) (3.4.3)\nRequirement already satisfied: urllib3<3,>=1.21.1 in /usr/local/lib/python3.11/dist-packages (from requests>=2.19.0->evaluate) (2.5.0)\nRequirement already satisfied: certifi>=2017.4.17 in /usr/local/lib/python3.11/dist-packages (from requests>=2.19.0->evaluate) (2025.8.3)\nRequirement already satisfied: intel-openmp<2026,>=2024 in /usr/local/lib/python3.11/dist-packages (from mkl->numpy>=1.17->evaluate) (2024.2.0)\nRequirement already satisfied: tbb==2022.* in /usr/local/lib/python3.11/dist-packages (from mkl->numpy>=1.17->evaluate) (2022.2.0)\nRequirement already satisfied: intel-cmplr-lib-ur==2024.2.0 in /usr/local/lib/python3.11/dist-packages (from intel-openmp<2026,>=2024->mkl->numpy>=1.17->evaluate) (2024.2.0)\nRequirement already satisfied: tcmlib==1.* in /usr/local/lib/python3.11/dist-packages (from tbb==2022.*->mkl->numpy>=1.17->evaluate) (1.4.0)\nRequirement already satisfied: intel-cmplr-lib-rt in /usr/local/lib/python3.11/dist-packages (from mkl_umath->numpy>=1.17->evaluate) (2024.2.0)\nRequirement already satisfied: python-dateutil>=2.8.2 in /usr/local/lib/python3.11/dist-packages (from pandas->evaluate) (2.9.0.post0)\nRequirement already satisfied: pytz>=2020.1 in /usr/local/lib/python3.11/dist-packages (from pandas->evaluate) (2025.2)\nRequirement already satisfied: tzdata>=2022.7 in /usr/local/lib/python3.11/dist-packages (from pandas->evaluate) (2025.2)\nRequirement already satisfied: six>=1.5 in /usr/local/lib/python3.11/dist-packages (from python-dateutil>=2.8.2->pandas->evaluate) (1.17.0)\nDownloading evaluate-0.4.6-py3-none-any.whl (84 kB)\nDownloading jiwer-4.0.0-py3-none-any.whl (23 kB)\nDownloading rapidfuzz-3.14.3-cp311-cp311-manylinux_2_27_x86_64.manylinux_2_28_x86_64.whl (3.2 MB)\n\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m3.2/3.2 MB\u001b[0m \u001b[31m32.2 MB/s\u001b[0m  \u001b[33m0:00:00\u001b[0m\n\u001b[?25hInstalling collected packages: rapidfuzz, jiwer, evaluate\n\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m3/3\u001b[0m [evaluate]2/3\u001b[0m [evaluate]\n\u001b[1A\u001b[2KSuccessfully installed evaluate-0.4.6 jiwer-4.0.0 rapidfuzz-3.14.3\nNote: you may need to restart the kernel to use updated packages.\n","output_type":"stream"}],"execution_count":10},{"cell_type":"code","source":"# pip list","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-11-06T16:23:34.017876Z","iopub.execute_input":"2025-11-06T16:23:34.018486Z","iopub.status.idle":"2025-11-06T16:23:34.022021Z","shell.execute_reply.started":"2025-11-06T16:23:34.018460Z","shell.execute_reply":"2025-11-06T16:23:34.021421Z"}},"outputs":[],"execution_count":11},{"cell_type":"code","source":"import datasets\nprint(datasets.__version__)","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-11-06T16:23:34.022729Z","iopub.execute_input":"2025-11-06T16:23:34.022903Z","iopub.status.idle":"2025-11-06T16:23:35.510390Z","shell.execute_reply.started":"2025-11-06T16:23:34.022889Z","shell.execute_reply":"2025-11-06T16:23:35.509564Z"}},"outputs":[{"name":"stdout","text":"4.1.1\n","output_type":"stream"}],"execution_count":12},{"cell_type":"code","source":"import os\nimport re\nimport json\nimport torch\nimport warnings\nwarnings.filterwarnings('ignore')\nimport numpy as np\nimport pandas as pd\nfrom pathlib import Path\nfrom dataclasses import dataclass\nfrom typing import Dict, List, Union, Any\nfrom tqdm.auto import tqdm\nimport librosa\nimport soundfile as sf\nfrom audiomentations import Compose, AddGaussianNoise, TimeStretch, PitchShift, Shift\nfrom datasets import Dataset, DatasetDict, Audio\nfrom transformers import (\n    WhisperFeatureExtractor,\n    WhisperTokenizer,\n    WhisperProcessor,\n    WhisperForConditionalGeneration,\n    Seq2SeqTrainingArguments,\n    Seq2SeqTrainer,\n    EarlyStoppingCallback\n)\nimport evaluate\nfrom transformers import BitsAndBytesConfig\nfrom peft import LoraConfig, get_peft_model, prepare_model_for_kbit_training\n\n\n# For visualization\nimport matplotlib.pyplot as plt\nimport seaborn as sns\nsns.set_style(\"whitegrid\")\n\n# GPU Check\nprint(f\"\\n{'='*60}\")\nprint(f\"🖥️  GPU Available: {torch.cuda.is_available()}\")\nif torch.cuda.is_available():\n    print(f\"📊 GPU: {torch.cuda.get_device_name(0)}\")\n    print(f\"💾 GPU Memory: {torch.cuda.get_device_properties(0).total_memory / 1e9:.2f} GB\")\nprint(f\"{'='*60}\\n\")\n","metadata":{"_uuid":"8f2839f25d086af736a60e9eeb907d3b93b6e0e5","_cell_guid":"b1076dfc-b9ad-4769-8c92-a6c4dae69d19","trusted":true,"execution":{"iopub.status.busy":"2025-11-06T16:25:26.986373Z","iopub.execute_input":"2025-11-06T16:25:26.987071Z","iopub.status.idle":"2025-11-06T16:25:50.423220Z","shell.execute_reply.started":"2025-11-06T16:25:26.987048Z","shell.execute_reply":"2025-11-06T16:25:50.422426Z"}},"outputs":[{"name":"stderr","text":"2025-11-06 16:25:35.632681: E external/local_xla/xla/stream_executor/cuda/cuda_fft.cc:477] Unable to register cuFFT factory: Attempting to register factory for plugin cuFFT when one has already been registered\nWARNING: All log messages before absl::InitializeLog() is called are written to STDERR\nE0000 00:00:1762446335.835298      47 cuda_dnn.cc:8310] Unable to register cuDNN factory: Attempting to register factory for plugin cuDNN when one has already been registered\nE0000 00:00:1762446335.889166      47 cuda_blas.cc:1418] Unable to register cuBLAS factory: Attempting to register factory for plugin cuBLAS when one has already been registered\n","output_type":"stream"},{"name":"stdout","text":"\n============================================================\n🖥️  GPU Available: True\n📊 GPU: Tesla P100-PCIE-16GB\n💾 GPU Memory: 17.06 GB\n============================================================\n\n","output_type":"stream"}],"execution_count":17},{"cell_type":"markdown","source":"# Dataset Loading and prasing","metadata":{}},{"cell_type":"markdown","source":"### Chunking long audio","metadata":{}},{"cell_type":"code","source":"def chunk_long_audio(audio_path, transcription, max_duration=30.0, overlap=1.0):\n    \"\"\"\n    Split long audio files into chunks of max_duration seconds.\n    \n    Args:\n        audio_path: Path to audio file\n        transcription: Full transcription text\n        max_duration: Maximum chunk duration in seconds (default 30s for Whisper)\n        overlap: Overlap between chunks in seconds to avoid cutting words\n    \n    Returns:\n        List of dicts with chunked audio paths and estimated transcriptions\n    \"\"\"\n    audio, sr = librosa.load(audio_path, sr=16000)\n    audio_duration = len(audio) / sr\n    \n    # If audio is short enough, return as-is\n    if audio_duration <= max_duration:\n        return [{'audio_path': audio_path, 'transcription': transcription}]\n    \n    # Calculate chunk parameters\n    chunk_samples = int(max_duration * sr)\n    overlap_samples = int(overlap * sr)\n    step_samples = chunk_samples - overlap_samples\n    \n    chunks = []\n    words = transcription.split()\n    total_chunks = int(np.ceil((len(audio) - chunk_samples) / step_samples)) + 1\n    words_per_chunk = max(1, len(words) // total_chunks)\n    \n    chunk_idx = 0\n    word_start = 0\n    \n    for start in range(0, len(audio) - overlap_samples, step_samples):\n        end = min(start + chunk_samples, len(audio))\n        chunk_audio = audio[start:end]\n        \n        # Estimate text for this chunk (proportional split)\n        word_end = min(word_start + words_per_chunk, len(words))\n        \n        # For last chunk, take remaining words\n        if start + step_samples >= len(audio) - chunk_samples:\n            word_end = len(words)\n        \n        chunk_text = ' '.join(words[word_start:word_end])\n        \n        # Save chunk temporarily\n        chunk_filename = f\"chunk_{chunk_idx}_{Path(audio_path).name}\"\n        chunk_path = f\"/kaggle/working/chunks/{chunk_filename}\"\n        os.makedirs(\"/kaggle/working/chunks\", exist_ok=True)\n        sf.write(chunk_path, chunk_audio, sr)\n        \n        chunks.append({\n            'audio_path': chunk_path,\n            'transcription': chunk_text.strip()\n        })\n        \n        word_start = word_end\n        chunk_idx += 1\n        \n        # Stop if we've processed all audio\n        if end >= len(audio):\n            break\n    \n    print(f\"  📌 Split {Path(audio_path).name} ({audio_duration:.1f}s) → {len(chunks)} chunks\")\n    return chunks","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-11-06T16:25:50.424511Z","iopub.execute_input":"2025-11-06T16:25:50.425250Z","iopub.status.idle":"2025-11-06T16:25:50.432823Z","shell.execute_reply.started":"2025-11-06T16:25:50.425220Z","shell.execute_reply":"2025-11-06T16:25:50.432099Z"}},"outputs":[],"execution_count":18},{"cell_type":"code","source":"def augment_audio(audio, sr):\n    augmenter = Compose([\n        AddGaussianNoise(min_amplitude=0.001, max_amplitude=0.015, p=0.5),\n        TimeStretch(min_rate=0.9, max_rate=1.1, p=0.5),\n        PitchShift(min_semitones=-2, max_semitones=2, p=0.5),\n        Shift(min_shift=-0.5, max_shift=0.5, p=0.3),\n    ])\n    \n    return augmenter(samples=audio, sample_rate=sr)","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-11-06T16:25:50.433637Z","iopub.execute_input":"2025-11-06T16:25:50.433858Z","iopub.status.idle":"2025-11-06T16:25:50.459627Z","shell.execute_reply.started":"2025-11-06T16:25:50.433833Z","shell.execute_reply":"2025-11-06T16:25:50.458920Z"}},"outputs":[],"execution_count":19},{"cell_type":"code","source":"def parse_text_file(text_path):\n    \"\"\"\n    Parse text files handling both formats:\n    - '1.Text here' or '1. Text here' → removes number prefix\n    - 'Text here' → uses as-is\n    Returns: list of (line_number, cleaned_text) tuples\n    \"\"\"\n    with open(text_path, 'r', encoding='utf-8') as f:\n        lines = f.readlines()\n    \n    parsed = []\n    for line in lines:\n        original = line.strip()\n        if not original:\n            continue\n            \n        # Extract line number if present: \"1.\" or \"100.\" etc\n        match = re.match(r'^(\\d+)\\.?\\s*(.+)$', original)\n        if match:\n            line_num = int(match.group(1))\n            text = match.group(2).strip()\n        else:\n            line_num = None\n            text = original\n        \n        parsed.append((line_num, text))\n    \n    return parsed\n\n\ndef extract_audio_number(filename):\n    \"\"\"\n    Extract number from various audio filename formats:\n    - 'Voice 1.mp3' → 1\n    - '1_load my wallet.mp3' → 1\n    - 'Standard recording 1.mp3' → 1\n    - 'recording_001.mp3' → 1\n    Returns: number or None\n    \"\"\"\n    # Try pattern: '1_text.mp3'\n    match = re.match(r'^(\\d+)_', filename)\n    if match:\n        return int(match.group(1))\n    \n    # Try pattern: 'text 1.mp3' or 'text_1.mp3'\n    match = re.search(r'[\\s_](\\d+)\\.mp3$', filename)\n    if match:\n        return int(match.group(1))\n    \n    # Try pattern: 'text001.mp3'\n    match = re.search(r'(\\d+)\\.mp3$', filename)\n    if match:\n        return int(match.group(1))\n    \n    return None\n\n\ndef create_dataset_manifest(base_path):\n    \"\"\"\n    Create audio-text pairs with intelligent matching.\n    Handles both numbered and sequential matching.\n    \"\"\"\n    \n    dataset_entries = []\n    \n    # Mapping: audio_folder → text_file\n    mappings = {\n        'Voice_memo': 'voice_text_dataset/voice_text_dataset/voice_memo_text_data.txt',\n        'chinese_accent': 'voice_text_dataset/voice_text_dataset/chinese_accent_text_data.txt',\n        'voice_record': 'voice_text_dataset/voice_text_dataset/voice_record_text_data.txt',\n        # 'my_voice': 'voice_text_dataset/voice_text_dataset/my_voice_text_data.txt'\n    }\n    \n    total_mismatches = 0\n    \n    for audio_dir, text_file in mappings.items():\n        audio_path = Path(base_path) / audio_dir\n        text_path = Path(base_path) / text_file\n        \n        if not audio_path.exists():\n            print(f\"⚠️  Skipping: {audio_dir} (not found)\")\n            continue\n        \n        if not text_path.exists():\n            print(f\"⚠️  Skipping: {text_file} (not found)\")\n            continue\n        \n        # Get audio files\n        audio_files = sorted(audio_path.glob('*.mp3'))\n        \n        # Parse transcriptions\n        transcriptions = parse_text_file(text_path)\n        \n        # Create mapping: number → text\n        text_dict = {}\n        for line_num, text in transcriptions:\n            if line_num:\n                text_dict[line_num] = text\n        \n        # Match audio files to transcriptions\n        matched = 0\n        for audio_file in audio_files:\n            # Try to extract number from filename\n            audio_num = extract_audio_number(audio_file.name)\n            \n            if audio_num and audio_num in text_dict:\n                # Number-based matching\n                transcription = text_dict[audio_num]\n                matched += 1\n            elif len(transcriptions) > 0:\n                # Sequential fallback (use first available)\n                line_num, transcription = transcriptions.pop(0)\n                matched += 1\n            else:\n                print(f\"❌ No transcription for: {audio_file.name}\")\n                total_mismatches += 1\n                continue\n            \n            dataset_entries.append({\n                'audio_path': str(audio_file),\n                'transcription': transcription,\n                'category': audio_dir,\n                'filename': audio_file.name\n            })\n        \n        print(f\"✅ {audio_dir}: {matched} files matched\")\n    \n    # Handle single file with multi-sentence transcription\n    single_file = Path(base_path) / 'my_voice_sample.mp3'\n    single_text = Path(base_path) / 'voice_text_dataset/voice_text_dataset/my_voice_text_data.txt'\n    \n    if single_file.exists() and single_text.exists():\n        with open(single_text, 'r', encoding='utf-8') as f:\n            # Split by sentence if multiple exist\n            text = f.read().strip()\n            sentences = re.split(r'[.!?]+\\s+', text)\n            # Use full text as one entry\n            full_text = ' '.join(sentences).strip()\n            \n        dataset_entries.append({\n            'audio_path': str(single_file),\n            'transcription': full_text,\n            'category': 'single_voice',\n            'filename': single_file.name\n        })\n        print(f\"✅ single_voice: 1 file matched\")\n    \n    print(f\"\\n{'='*60}\")\n    print(f\"📊 Total matched: {len(dataset_entries)} samples\")\n    if total_mismatches > 0:\n        print(f\"⚠️  Mismatches: {total_mismatches} files\")\n    print(f\"{'='*60}\\n\")\n    \n    return dataset_entries\n\n\n# Create manifest\nBASE_PATH = '/kaggle/input/fine-tuning-dataset'\nprint(\"🔍 Scanning dataset...\\n\")\ndataset_manifest = create_dataset_manifest(BASE_PATH)\n\n# Convert to DataFrame\ndf = pd.DataFrame(dataset_manifest)\nprint(\"\\n📋 Dataset Breakdown:\")\nprint(df.groupby('category').size())\nprint(f\"\\n💾 Saving manifest...\")\ndf.to_csv('dataset_manifest.csv', index=False)\nprint(\"✅ Saved to dataset_manifest.csv\")\n\n# Display sample\nprint(\"\\n📝 Sample entries:\")\nprint(df.head(3)[['filename', 'transcription', 'category']])\n","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-11-06T16:27:33.441323Z","iopub.execute_input":"2025-11-06T16:27:33.441675Z","iopub.status.idle":"2025-11-06T16:27:33.609813Z","shell.execute_reply.started":"2025-11-06T16:27:33.441652Z","shell.execute_reply":"2025-11-06T16:27:33.608969Z"}},"outputs":[{"name":"stdout","text":"🔍 Scanning dataset...\n\n✅ Voice_memo: 101 files matched\n✅ chinese_accent: 100 files matched\n✅ voice_record: 92 files matched\n✅ single_voice: 1 file matched\n\n============================================================\n📊 Total matched: 294 samples\n============================================================\n\n\n📋 Dataset Breakdown:\ncategory\nVoice_memo        101\nchinese_accent    100\nsingle_voice        1\nvoice_record       92\ndtype: int64\n\n💾 Saving manifest...\n✅ Saved to dataset_manifest.csv\n\n📝 Sample entries:\n        filename                        transcription    category\n0    Voice 1.mp3  I want to topup amount to my wallet  Voice_memo\n1   Voice 10.mp3       Load 150 rupees into my wallet  Voice_memo\n2  Voice 100.mp3           Add 900 using card payment  Voice_memo\n","output_type":"stream"}],"execution_count":20},{"cell_type":"markdown","source":"# Audio Validation & Duration Calculation","metadata":{}},{"cell_type":"code","source":"import subprocess\n\ndef get_audio_duration(audio_path):\n    \"\"\"Get duration in seconds using FFmpeg\"\"\"\n    try:\n        result = subprocess.run(\n            ['ffprobe', '-v', 'error', '-show_entries', 'format=duration',\n             '-of', 'default=noprint_wrappers=1:nokey=1', audio_path],\n            stdout=subprocess.PIPE, stderr=subprocess.STDOUT, text=True\n        )\n        if result.returncode != 0:\n            print(f\"❌ FFmpeg error for {audio_path}: {result.stdout.strip()}\")\n            return 0\n        return float(result.stdout.strip())\n    except Exception as e:\n        print(f\"❌ Error processing {audio_path}: {e}\")\n        return 0\nprint(\"\\n⏱️  Calculating total duration...\")\ndf['duration'] = df['audio_path'].apply(get_audio_duration)\n\ntotal_duration_sec = df['duration'].sum()\ntotal_duration_min = total_duration_sec / 60\ntotal_duration_hr = total_duration_min / 60\n\nprint(f\"\\n{'='*60}\")\nprint(f\"📊 Dataset Statistics:\")\nprint(f\"   Total Samples: {len(df)}\")\nprint(f\"   Total Duration: {total_duration_min:.1f} minutes ({total_duration_hr:.2f} hours)\")\nprint(f\"   Avg Duration: {df['duration'].mean():.1f} seconds\")\nprint(f\"   Min Duration: {df['duration'].min():.1f} seconds\")\nprint(f\"   Max Duration: {df['duration'].max():.1f} seconds\")\nprint(f\"{'='*60}\\n\")\n\nif total_duration_hr < 0.5:\n    print(\"⚠️  WARNING: Dataset < 0.5 hours\")\n    print(\"   Expected: Overfitting likely, limited generalization\")\n    print(\"   Recommendation: Collect 10-20 more hours for production use\")\n    print(\"   Proceeding with POC training...\\n\")\n","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-11-06T16:28:08.038415Z","iopub.execute_input":"2025-11-06T16:28:08.038745Z","iopub.status.idle":"2025-11-06T16:28:37.456379Z","shell.execute_reply.started":"2025-11-06T16:28:08.038724Z","shell.execute_reply":"2025-11-06T16:28:37.455658Z"}},"outputs":[{"name":"stdout","text":"\n⏱️  Calculating total duration...\n\n============================================================\n📊 Dataset Statistics:\n   Total Samples: 294\n   Total Duration: 21.7 minutes (0.36 hours)\n   Avg Duration: 4.4 seconds\n   Min Duration: 1.3 seconds\n   Max Duration: 325.7 seconds\n============================================================\n\n⚠️  WARNING: Dataset < 0.5 hours\n   Expected: Overfitting likely, limited generalization\n   Recommendation: Collect 10-20 more hours for production use\n   Proceeding with POC training...\n\n","output_type":"stream"}],"execution_count":21},{"cell_type":"markdown","source":"# Data Augmentation (Critical for Small Datasets)","metadata":{}},{"cell_type":"code","source":"!apt-get update -qq && apt-get install -y ffmpeg","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-11-06T16:28:37.457565Z","iopub.execute_input":"2025-11-06T16:28:37.457810Z","iopub.status.idle":"2025-11-06T16:28:43.323683Z","shell.execute_reply.started":"2025-11-06T16:28:37.457787Z","shell.execute_reply":"2025-11-06T16:28:43.322676Z"}},"outputs":[{"name":"stdout","text":"W: Skipping acquire of configured file 'main/source/Sources' as repository 'https://r2u.stat.illinois.edu/ubuntu jammy InRelease' does not seem to provide it (sources.list entry misspelt?)\nReading package lists... Done\nBuilding dependency tree... Done\nReading state information... Done\nffmpeg is already the newest version (7:4.4.2-0ubuntu0.22.04.1).\n0 upgraded, 0 newly installed, 0 to remove and 171 not upgraded.\n","output_type":"stream"}],"execution_count":22},{"cell_type":"code","source":"def augment_audio(audio, sr):\n    \"\"\"\n    Apply audio augmentations to increase dataset diversity.\n    Helps combat overfitting on small datasets.\n    \"\"\"\n    augmenter = Compose([\n        AddGaussianNoise(min_amplitude=0.001, max_amplitude=0.015, p=0.5),\n        TimeStretch(min_rate=0.9, max_rate=1.1, p=0.5),\n        PitchShift(min_semitones=-2, max_semitones=2, p=0.5),\n        Shift(min_shift=-0.5, max_shift=0.5, p=0.3),\n    ])\n    \n    return augmenter(samples=audio, sample_rate=sr)\n\n\ndef create_augmented_dataset(df, augmentation_factor=2):\n    \"\"\"\n    Create augmented copies of dataset with audio chunking for long files.\n    \"\"\"\n    augmented_entries = []\n    \n    print(f\"🔄 Creating {augmentation_factor}x augmented dataset with chunking...\")\n    \n    for idx, row in tqdm(df.iterrows(), total=len(df)):\n        # Check if audio needs chunking (>30s)\n        if row['duration'] > 30.0:\n            print(f\"\\n⚠️  Long audio detected: {row['filename']} ({row['duration']:.1f}s)\")\n            chunks = chunk_long_audio(row['audio_path'], row['transcription'], max_duration=30.0)\n            \n            # Add all chunks\n            for chunk in chunks:\n                chunk_entry = row.to_dict()\n                chunk_entry['audio_path'] = chunk['audio_path']\n                chunk_entry['transcription'] = chunk['transcription']\n                chunk_entry['filename'] = Path(chunk['audio_path']).name\n                chunk_entry['duration'] = librosa.get_duration(path=chunk['audio_path'])\n                augmented_entries.append(chunk_entry)\n            \n            # Apply augmentation to chunks if needed\n            for aug_idx in range(augmentation_factor - 1):\n                for chunk in chunks:\n                    audio, sr = librosa.load(chunk['audio_path'], sr=16000)\n                    aug_audio = augment_audio(audio, sr)\n                    \n                    aug_filename = f\"aug_{aug_idx}_{Path(chunk['audio_path']).name}\"\n                    aug_path = f\"/kaggle/working/augmented/{aug_filename}\"\n                    os.makedirs(\"/kaggle/working/augmented\", exist_ok=True)\n                    sf.write(aug_path, aug_audio, sr)\n                    \n                    aug_entry = row.to_dict()\n                    aug_entry['audio_path'] = aug_path\n                    aug_entry['transcription'] = chunk['transcription']\n                    aug_entry['category'] = f\"{row['category']}_aug\"\n                    aug_entry['filename'] = aug_filename\n                    aug_entry['duration'] = len(aug_audio) / sr\n                    augmented_entries.append(aug_entry)\n        else:\n            # Normal processing for short audio\n            augmented_entries.append(row.to_dict())\n            \n            for aug_idx in range(augmentation_factor - 1):\n                audio, sr = librosa.load(row['audio_path'], sr=16000)\n                aug_audio = augment_audio(audio, sr)\n                \n                aug_filename = f\"aug_{aug_idx}_{Path(row['audio_path']).name}\"\n                aug_path = f\"/kaggle/working/augmented/{aug_filename}\"\n                os.makedirs(\"/kaggle/working/augmented\", exist_ok=True)\n                sf.write(aug_path, aug_audio, sr)\n                \n                augmented_entries.append({\n                    'audio_path': aug_path,\n                    'transcription': row['transcription'],\n                    'category': f\"{row['category']}_aug\",\n                    'filename': aug_filename,\n                    'duration': len(aug_audio) / sr\n                })\n    \n    return pd.DataFrame(augmented_entries)","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-11-06T16:28:43.324862Z","iopub.execute_input":"2025-11-06T16:28:43.325135Z","iopub.status.idle":"2025-11-06T16:28:43.337562Z","shell.execute_reply.started":"2025-11-06T16:28:43.325083Z","shell.execute_reply":"2025-11-06T16:28:43.336631Z"}},"outputs":[],"execution_count":23},{"cell_type":"code","source":"# Existing augmentation code...\ndf_augmented = create_augmented_dataset(df, augmentation_factor=2)\n\n# Add base_category for stratification (strips '_aug')\ndf_augmented['base_category'] = df_augmented['category'].str.replace('_aug', '')\n\n# Optional: Verify counts on base_category (all should now >=2)\nprint(\"📊 Base Category Counts:\")\nprint(df_augmented['base_category'].value_counts())\nprint(\"\\nBase Categories with <2 samples:\", df_augmented['base_category'].value_counts()[df_augmented['base_category'].value_counts() < 2].index.tolist())","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-11-06T16:28:43.339443Z","iopub.execute_input":"2025-11-06T16:28:43.339745Z","iopub.status.idle":"2025-11-06T16:29:24.275724Z","shell.execute_reply.started":"2025-11-06T16:28:43.339722Z","shell.execute_reply":"2025-11-06T16:29:24.274901Z"}},"outputs":[{"name":"stdout","text":"🔄 Creating 2x augmented dataset with chunking...\n","output_type":"stream"},{"output_type":"display_data","data":{"text/plain":"  0%|          | 0/294 [00:00<?, ?it/s]","application/vnd.jupyter.widget-view+json":{"version_major":2,"version_minor":0,"model_id":"bafc8840221d4a778a914140ddff43e2"}},"metadata":{}},{"name":"stderr","text":"Note: Illegal Audio-MPEG-Header 0x55555555 at offset 120960.\nNote: Trying to resync...\nNote: Skipped 868 bytes in input.\nNote: Illegal Audio-MPEG-Header 0x37393236 at offset 108480.\nNote: Trying to resync...\nNote: Hit end of (available) data during resync.\nNote: Illegal Audio-MPEG-Header 0x31352523 at offset 140160.\nNote: Trying to resync...\nNote: Hit end of (available) data during resync.\nNote: Illegal Audio-MPEG-Header 0x372c3130 at offset 101760.\nNote: Trying to resync...\nNote: Skipped 105 bytes in input.\nNote: Illegal Audio-MPEG-Header 0xb059ebe7 at offset 127680.\nNote: Trying to resync...\nNote: Hit end of (available) data during resync.\nNote: Illegal Audio-MPEG-Header 0x32393238 at offset 113280.\nNote: Trying to resync...\nNote: Hit end of (available) data during resync.\nNote: Illegal Audio-MPEG-Header 0x38363736 at offset 149760.\nNote: Trying to resync...\nNote: Hit end of (available) data during resync.\nNote: Illegal Audio-MPEG-Header 0x322c3138 at offset 133440.\nNote: Trying to resync...\nNote: Hit end of (available) data during resync.\nNote: Illegal Audio-MPEG-Header 0x32383337 at offset 108480.\nNote: Trying to resync...\nNote: Hit end of (available) data during resync.\nNote: Illegal Audio-MPEG-Header 0x5ec44e51 at offset 90240.\nNote: Trying to resync...\nNote: Skipped 79 bytes in input.\nNote: Illegal Audio-MPEG-Header 0x5b43697d at offset 84480.\nNote: Trying to resync...\nNote: Hit end of (available) data during resync.\nNote: Illegal Audio-MPEG-Header 0x39382c32 at offset 135360.\nNote: Trying to resync...\nNote: Skipped 146 bytes in input.\nNote: Illegal Audio-MPEG-Header 0xbcb11e6b at offset 70080.\nNote: Trying to resync...\nNote: Hit end of (available) data during resync.\nNote: Illegal Audio-MPEG-Header 0x30393934 at offset 51840.\nNote: Trying to resync...\nNote: Hit end of (available) data during resync.\nNote: Illegal Audio-MPEG-Header 0xc719dc7b at offset 99840.\nNote: Trying to resync...\nNote: Hit end of (available) data during resync.\nNote: Illegal Audio-MPEG-Header 0x3034372c at offset 66240.\nNote: Trying to resync...\nNote: Skipped 112 bytes in input.\nNote: Illegal Audio-MPEG-Header 0x7fe60bb2 at offset 79680.\nNote: Trying to resync...\nNote: Hit end of (available) data during resync.\nNote: Illegal Audio-MPEG-Header 0x9e6abd96 at offset 103680.\nNote: Trying to resync...\nNote: Hit end of (available) data during resync.\nNote: Illegal Audio-MPEG-Header 0x38342c38 at offset 60480.\nNote: Trying to resync...\nNote: Hit end of (available) data during resync.\nNote: Illegal Audio-MPEG-Header 0x7cd47323 at offset 111360.\nNote: Trying to resync...\nNote: Hit end of (available) data during resync.\nNote: Illegal Audio-MPEG-Header 0x38363731 at offset 87360.\nNote: Trying to resync...\nNote: Skipped 242 bytes in input.\nNote: Illegal Audio-MPEG-Header 0x3332312c at offset 89280.\nNote: Trying to resync...\nNote: Hit end of (available) data during resync.\nNote: Illegal Audio-MPEG-Header 0x306d6172 at offset 99840.\nNote: Trying to resync...\nNote: Skipped 93 bytes in input.\nNote: Illegal Audio-MPEG-Header 0x392c3239 at offset 84480.\nNote: Trying to resync...\nNote: Hit end of (available) data during resync.\nNote: Illegal Audio-MPEG-Header 0x312c3239 at offset 77760.\nNote: Trying to resync...\nNote: Hit end of (available) data during resync.\nNote: Illegal Audio-MPEG-Header 0x34342c32 at offset 80640.\nNote: Trying to resync...\nNote: Skipped 217 bytes in input.\nNote: Illegal Audio-MPEG-Header 0x2c323636 at offset 87360.\nNote: Trying to resync...\nNote: Skipped 226 bytes in input.\nNote: Illegal Audio-MPEG-Header 0xa6b78be7 at offset 99840.\nNote: Trying to resync...\nNote: Hit end of (available) data during resync.\nNote: Illegal Audio-MPEG-Header 0x3546f8ea at offset 84480.\nNote: Trying to resync...\nNote: Hit end of (available) data during resync.\nNote: Illegal Audio-MPEG-Header 0x3cb8fb92 at offset 95040.\nNote: Trying to resync...\nNote: Hit end of (available) data during resync.\nNote: Illegal Audio-MPEG-Header 0x342c3134 at offset 108480.\nNote: Trying to resync...\nNote: Skipped 242 bytes in input.\nNote: Illegal Audio-MPEG-Header 0x614e4daa at offset 77760.\nNote: Trying to resync...\nNote: Hit end of (available) data during resync.\nNote: Illegal Audio-MPEG-Header 0x2c313131 at offset 80640.\nNote: Trying to resync...\nNote: Skipped 115 bytes in input.\nNote: Illegal Audio-MPEG-Header 0x2c323835 at offset 127680.\nNote: Trying to resync...\nNote: Hit end of (available) data during resync.\nNote: Illegal Audio-MPEG-Header 0x3337342c at offset 132480.\nNote: Trying to resync...\nNote: Hit end of (available) data during resync.\nNote: Illegal Audio-MPEG-Header 0x2c323932 at offset 109440.\nNote: Trying to resync...\nNote: Hit end of (available) data during resync.\nNote: Illegal Audio-MPEG-Header 0x342c3239 at offset 114240.\nNote: Trying to resync...\nNote: Hit end of (available) data during resync.\nNote: Illegal Audio-MPEG-Header 0x322c3238 at offset 122880.\nNote: Trying to resync...\nNote: Hit end of (available) data during resync.\nNote: Illegal Audio-MPEG-Header 0x3833362c at offset 111360.\nNote: Trying to resync...\nNote: Hit end of (available) data during resync.\nNote: Illegal Audio-MPEG-Header 0x352c3238 at offset 113280.\nNote: Trying to resync...\nNote: Hit end of (available) data during resync.\nNote: Illegal Audio-MPEG-Header 0x2c323336 at offset 120960.\nNote: Trying to resync...\nNote: Hit end of (available) data during resync.\nNote: Illegal Audio-MPEG-Header 0x2c323931 at offset 99840.\nNote: Trying to resync...\nNote: Skipped 159 bytes in input.\nNote: Illegal Audio-MPEG-Header 0x35352c32 at offset 96960.\nNote: Trying to resync...\nNote: Hit end of (available) data during resync.\nNote: Illegal Audio-MPEG-Header 0x302c3239 at offset 104640.\nNote: Trying to resync...\nNote: Hit end of (available) data during resync.\nNote: Illegal Audio-MPEG-Header 0x3038332c at offset 135360.\nNote: Trying to resync...\nNote: Skipped 357 bytes in input.\nNote: Illegal Audio-MPEG-Header 0x2c323932 at offset 114240.\nNote: Trying to resync...\nNote: Hit end of (available) data during resync.\nNote: Illegal Audio-MPEG-Header 0x939c1854 at offset 113280.\nNote: Trying to resync...\nNote: Hit end of (available) data during resync.\nNote: Illegal Audio-MPEG-Header 0x38342c31 at offset 101760.\nNote: Trying to resync...\nNote: Hit end of (available) data during resync.\nNote: Illegal Audio-MPEG-Header 0xae432b1e at offset 120960.\nNote: Trying to resync...\nNote: Hit end of (available) data during resync.\n","output_type":"stream"},{"name":"stdout","text":"\n⚠️  Long audio detected: my_voice_sample.mp3 (325.7s)\n  📌 Split my_voice_sample.mp3 (325.6s) → 12 chunks\n📊 Base Category Counts:\nbase_category\nVoice_memo        202\nchinese_accent    200\nvoice_record      184\nsingle_voice       24\nName: count, dtype: int64\n\nBase Categories with <2 samples: []\n","output_type":"stream"}],"execution_count":24},{"cell_type":"code","source":"len(df_augmented)","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-11-06T16:29:24.276593Z","iopub.execute_input":"2025-11-06T16:29:24.276881Z","iopub.status.idle":"2025-11-06T16:29:24.282568Z","shell.execute_reply.started":"2025-11-06T16:29:24.276861Z","shell.execute_reply":"2025-11-06T16:29:24.281803Z"}},"outputs":[{"execution_count":25,"output_type":"execute_result","data":{"text/plain":"610"},"metadata":{}}],"execution_count":25},{"cell_type":"markdown","source":"# train test split","metadata":{}},{"cell_type":"code","source":"from sklearn.model_selection import train_test_split\n\n# 80% train, 10% val, 10% test\ntrain_df, temp_df = train_test_split(df_augmented, test_size=0.2, random_state=42, \n                                      stratify=df_augmented['base_category'])\nval_df, test_df = train_test_split(temp_df, test_size=0.5, random_state=42,\n                                    stratify=temp_df['base_category'])\n\nprint(f\"📊 Dataset Split:\")\nprint(f\"   Train: {len(train_df)} samples ({len(train_df)/len(df_augmented)*100:.1f}%)\")\nprint(f\"   Val:   {len(val_df)} samples ({len(val_df)/len(df_augmented)*100:.1f}%)\")\nprint(f\"   Test:  {len(test_df)} samples ({len(test_df)/len(df_augmented)*100:.1f}%)\\n\")","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-11-06T16:29:24.283470Z","iopub.execute_input":"2025-11-06T16:29:24.283690Z","iopub.status.idle":"2025-11-06T16:29:24.307360Z","shell.execute_reply.started":"2025-11-06T16:29:24.283667Z","shell.execute_reply":"2025-11-06T16:29:24.306555Z"}},"outputs":[{"name":"stdout","text":"📊 Dataset Split:\n   Train: 488 samples (80.0%)\n   Val:   61 samples (10.0%)\n   Test:  61 samples (10.0%)\n\n","output_type":"stream"}],"execution_count":26},{"cell_type":"markdown","source":"# loading the model","metadata":{}},{"cell_type":"code","source":"print(\"📥 Loading Whisper Large-v3...\\n\")\n\nmodel_name = \"openai/whisper-large-v3\"\n\nfeature_extractor = WhisperFeatureExtractor.from_pretrained(model_name)\ntokenizer = WhisperTokenizer.from_pretrained(model_name, language=\"English\", task=\"transcribe\")\nprocessor = WhisperProcessor.from_pretrained(model_name, language=\"English\", task=\"transcribe\")\n\nbnb_config = BitsAndBytesConfig(\n    load_in_4bit=True,\n    bnb_4bit_quant_type=\"nf4\",\n    bnb_4bit_compute_dtype=torch.float16,\n    bnb_4bit_use_double_quant=True  # Optional: nested quantization for extra memory savings\n)\n\nmodel = WhisperForConditionalGeneration.from_pretrained(\n    model_name,\n    quantization_config=bnb_config,\n    device_map=\"auto\"\n)\n\n\nmodel.generation_config.language = \"english\"\nmodel.generation_config.task = \"transcribe\"\nmodel.generation_config.forced_decoder_ids = None\n\nprint(\"✅ Model loaded!\\n\")","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-11-06T16:29:24.308228Z","iopub.execute_input":"2025-11-06T16:29:24.308500Z","iopub.status.idle":"2025-11-06T16:29:57.152387Z","shell.execute_reply.started":"2025-11-06T16:29:24.308477Z","shell.execute_reply":"2025-11-06T16:29:57.151486Z"}},"outputs":[{"name":"stdout","text":"📥 Loading Whisper Large-v3...\n\n","output_type":"stream"},{"output_type":"display_data","data":{"text/plain":"preprocessor_config.json:   0%|          | 0.00/340 [00:00<?, ?B/s]","application/vnd.jupyter.widget-view+json":{"version_major":2,"version_minor":0,"model_id":"4c809ade0d934e57ab6a3830061f8623"}},"metadata":{}},{"output_type":"display_data","data":{"text/plain":"tokenizer_config.json: 0.00B [00:00, ?B/s]","application/vnd.jupyter.widget-view+json":{"version_major":2,"version_minor":0,"model_id":"b2126a973ed54e39ab7e80ea6b46b070"}},"metadata":{}},{"output_type":"display_data","data":{"text/plain":"vocab.json: 0.00B [00:00, ?B/s]","application/vnd.jupyter.widget-view+json":{"version_major":2,"version_minor":0,"model_id":"f76868fd38644ca986cbdddb70d676b9"}},"metadata":{}},{"output_type":"display_data","data":{"text/plain":"tokenizer.json: 0.00B [00:00, ?B/s]","application/vnd.jupyter.widget-view+json":{"version_major":2,"version_minor":0,"model_id":"dcace5f498c048a2ac8f095caab5cf3e"}},"metadata":{}},{"output_type":"display_data","data":{"text/plain":"merges.txt: 0.00B [00:00, ?B/s]","application/vnd.jupyter.widget-view+json":{"version_major":2,"version_minor":0,"model_id":"5e9bb2ff5462440a97dee379cf9c385a"}},"metadata":{}},{"output_type":"display_data","data":{"text/plain":"normalizer.json: 0.00B [00:00, ?B/s]","application/vnd.jupyter.widget-view+json":{"version_major":2,"version_minor":0,"model_id":"85a9561444964d7ca85f22fae24b0fdc"}},"metadata":{}},{"output_type":"display_data","data":{"text/plain":"added_tokens.json: 0.00B [00:00, ?B/s]","application/vnd.jupyter.widget-view+json":{"version_major":2,"version_minor":0,"model_id":"e361f5aaaa244b408a279e430e609d5d"}},"metadata":{}},{"output_type":"display_data","data":{"text/plain":"special_tokens_map.json: 0.00B [00:00, ?B/s]","application/vnd.jupyter.widget-view+json":{"version_major":2,"version_minor":0,"model_id":"345197014ffe4ffba37c50e70c3d9db4"}},"metadata":{}},{"output_type":"display_data","data":{"text/plain":"config.json: 0.00B [00:00, ?B/s]","application/vnd.jupyter.widget-view+json":{"version_major":2,"version_minor":0,"model_id":"215297b585f94b6cac9869d5903a4278"}},"metadata":{}},{"output_type":"display_data","data":{"text/plain":"model.safetensors:   0%|          | 0.00/3.09G [00:00<?, ?B/s]","application/vnd.jupyter.widget-view+json":{"version_major":2,"version_minor":0,"model_id":"55025f6ef5b74c598f9826697a05c5f8"}},"metadata":{}},{"output_type":"display_data","data":{"text/plain":"generation_config.json: 0.00B [00:00, ?B/s]","application/vnd.jupyter.widget-view+json":{"version_major":2,"version_minor":0,"model_id":"ed22a4df7edc4f2db052d34796e0750b"}},"metadata":{}},{"name":"stdout","text":"✅ Model loaded!\n\n","output_type":"stream"}],"execution_count":27},{"cell_type":"markdown","source":"# applying lora confriguation","metadata":{}},{"cell_type":"code","source":"print(\"🔧 Applying LoRA configuration...\\n\")\n# Prepare model for LoRA training\nmodel = prepare_model_for_kbit_training(model)\n# Optionally freeze the encoder to focus LoRA on decoder only (for efficiency)\nmodel.model.encoder.requires_grad_(False)\n# LoRA Configuration for Whisper\nlora_config = LoraConfig(\n    r=32,                          # LoRA rank (higher = more parameters, better quality)\n    lora_alpha=64,                 # LoRA scaling factor\n    target_modules=[               # Use simple suffixes to match all relevant layers (applies to both encoder/decoder)\n        \"q_proj\",\n        \"v_proj\",\n        \"k_proj\",\n        \"out_proj\",\n        \"fc1\",\n        \"fc2\"\n    ],\n    lora_dropout=0.05,             # Dropout for LoRA layers\n    bias=\"none\",                   # Don't train biases\n    # task_type=\"SEQ_2_SEQ_LM\"       # Critical fix: Use seq2seq for Whisper\n)\n# Apply LoRA to model\nmodel = get_peft_model(model, lora_config)\n# Print trainable parameters\ntrainable_params = sum(p.numel() for p in model.parameters() if p.requires_grad)\ntotal_params = sum(p.numel() for p in model.parameters())\nprint(f\"{'='*60}\")\nprint(f\"📊 LoRA Model Statistics:\")\nprint(f\"   Trainable params: {trainable_params:,} ({trainable_params/total_params*100:.2f}%)\")\nprint(f\"   Total params: {total_params:,}\")\nprint(f\"   Memory reduction: ~{100 - (trainable_params/total_params*100):.1f}%\")\nprint(f\"{'='*60}\\n\")","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-11-06T16:29:57.153983Z","iopub.execute_input":"2025-11-06T16:29:57.154239Z","iopub.status.idle":"2025-11-06T16:29:58.703391Z","shell.execute_reply.started":"2025-11-06T16:29:57.154222Z","shell.execute_reply":"2025-11-06T16:29:58.702518Z"}},"outputs":[{"name":"stdout","text":"🔧 Applying LoRA configuration...\n\n============================================================\n📊 LoRA Model Statistics:\n   Trainable params: 57,671,680 (6.65%)\n   Total params: 867,159,040\n   Memory reduction: ~93.3%\n============================================================\n\n","output_type":"stream"}],"execution_count":28},{"cell_type":"markdown","source":"# Prepare Dataset for Training (SAME AS BEFORE)\n","metadata":{}},{"cell_type":"code","source":"def prepare_dataset_entry(batch):\n    \"\"\"\n    Prepare dataset entry with token length validation.\n    \"\"\"\n    audio, sr = librosa.load(batch[\"audio_path\"], sr=16000)\n    \n    batch[\"input_features\"] = processor.feature_extractor(\n        audio, sampling_rate=16000\n    ).input_features[0]\n    \n    # Tokenize with truncation as safety measure\n    labels = tokenizer(\n        batch[\"transcription\"],\n        truncation=True,\n        max_length=448  # Whisper's max token length\n    ).input_ids\n    \n    batch[\"labels\"] = labels\n    \n    # Warning if truncation occurred\n    if len(labels) >= 448:\n        print(f\"⚠️  Truncated labels for: {batch.get('audio_path', 'unknown')} (originally {len(labels)} tokens)\")\n    \n    return batch\n\n\ndef df_to_dataset(df):\n    return Dataset.from_dict({\n        \"audio_path\": df[\"audio_path\"].tolist(),\n        \"transcription\": df[\"transcription\"].tolist()\n    })\n\nprint(\"🔄 Processing dataset...\")\ntrain_dataset = df_to_dataset(train_df).map(prepare_dataset_entry, remove_columns=[\"audio_path\", \"transcription\"])\nval_dataset = df_to_dataset(val_df).map(prepare_dataset_entry, remove_columns=[\"audio_path\", \"transcription\"])\ntest_dataset = df_to_dataset(test_df).map(prepare_dataset_entry, remove_columns=[\"audio_path\", \"transcription\"])\n\nprint(\"✅ Dataset ready!\\n\")\n","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-11-06T16:29:58.704343Z","iopub.execute_input":"2025-11-06T16:29:58.704682Z","iopub.status.idle":"2025-11-06T16:30:21.662569Z","shell.execute_reply.started":"2025-11-06T16:29:58.704656Z","shell.execute_reply":"2025-11-06T16:30:21.661562Z"}},"outputs":[{"name":"stdout","text":"🔄 Processing dataset...\n","output_type":"stream"},{"output_type":"display_data","data":{"text/plain":"Map:   0%|          | 0/488 [00:00<?, ? examples/s]","application/vnd.jupyter.widget-view+json":{"version_major":2,"version_minor":0,"model_id":"deacae31c9814ca2a1127104c68441ea"}},"metadata":{}},{"name":"stderr","text":"Note: Illegal Audio-MPEG-Header 0x342c3134 at offset 108480.\nNote: Trying to resync...\nNote: Skipped 242 bytes in input.\nNote: Illegal Audio-MPEG-Header 0x32393238 at offset 113280.\nNote: Trying to resync...\nNote: Hit end of (available) data during resync.\nNote: Illegal Audio-MPEG-Header 0x3034372c at offset 66240.\nNote: Trying to resync...\nNote: Skipped 112 bytes in input.\nNote: Illegal Audio-MPEG-Header 0x2c323932 at offset 109440.\nNote: Trying to resync...\nNote: Hit end of (available) data during resync.\nNote: Illegal Audio-MPEG-Header 0x939c1854 at offset 113280.\nNote: Trying to resync...\nNote: Hit end of (available) data during resync.\nNote: Illegal Audio-MPEG-Header 0x30393934 at offset 51840.\nNote: Trying to resync...\nNote: Hit end of (available) data during resync.\nNote: Illegal Audio-MPEG-Header 0x3833362c at offset 111360.\nNote: Trying to resync...\nNote: Hit end of (available) data during resync.\nNote: Illegal Audio-MPEG-Header 0x31352523 at offset 140160.\nNote: Trying to resync...\nNote: Hit end of (available) data during resync.\nNote: Illegal Audio-MPEG-Header 0x5b43697d at offset 84480.\nNote: Trying to resync...\nNote: Hit end of (available) data during resync.\nNote: Illegal Audio-MPEG-Header 0x2c313131 at offset 80640.\nNote: Trying to resync...\nNote: Skipped 115 bytes in input.\nNote: Illegal Audio-MPEG-Header 0xae432b1e at offset 120960.\nNote: Trying to resync...\nNote: Hit end of (available) data during resync.\nNote: Illegal Audio-MPEG-Header 0x352c3238 at offset 113280.\nNote: Trying to resync...\nNote: Hit end of (available) data during resync.\nNote: Illegal Audio-MPEG-Header 0x302c3239 at offset 104640.\nNote: Trying to resync...\nNote: Hit end of (available) data during resync.\nNote: Illegal Audio-MPEG-Header 0x2c323835 at offset 127680.\nNote: Trying to resync...\nNote: Hit end of (available) data during resync.\nNote: Illegal Audio-MPEG-Header 0x2c323636 at offset 87360.\nNote: Trying to resync...\nNote: Skipped 226 bytes in input.\nNote: Illegal Audio-MPEG-Header 0x5ec44e51 at offset 90240.\nNote: Trying to resync...\nNote: Skipped 79 bytes in input.\nNote: Illegal Audio-MPEG-Header 0x55555555 at offset 120960.\nNote: Trying to resync...\nNote: Skipped 868 bytes in input.\nNote: Illegal Audio-MPEG-Header 0x38363736 at offset 149760.\nNote: Trying to resync...\nNote: Hit end of (available) data during resync.\nNote: Illegal Audio-MPEG-Header 0xa6b78be7 at offset 99840.\nNote: Trying to resync...\nNote: Hit end of (available) data during resync.\nNote: Illegal Audio-MPEG-Header 0xbcb11e6b at offset 70080.\nNote: Trying to resync...\nNote: Hit end of (available) data during resync.\nNote: Illegal Audio-MPEG-Header 0x306d6172 at offset 99840.\nNote: Trying to resync...\nNote: Skipped 93 bytes in input.\nNote: Illegal Audio-MPEG-Header 0x2c323336 at offset 120960.\nNote: Trying to resync...\nNote: Hit end of (available) data during resync.\nNote: Illegal Audio-MPEG-Header 0x3546f8ea at offset 84480.\nNote: Trying to resync...\nNote: Hit end of (available) data during resync.\nNote: Illegal Audio-MPEG-Header 0x9e6abd96 at offset 103680.\nNote: Trying to resync...\nNote: Hit end of (available) data during resync.\nNote: Illegal Audio-MPEG-Header 0x3332312c at offset 89280.\nNote: Trying to resync...\nNote: Hit end of (available) data during resync.\nNote: Illegal Audio-MPEG-Header 0x312c3239 at offset 77760.\nNote: Trying to resync...\nNote: Hit end of (available) data during resync.\nNote: Illegal Audio-MPEG-Header 0x39382c32 at offset 135360.\nNote: Trying to resync...\nNote: Skipped 146 bytes in input.\nNote: Illegal Audio-MPEG-Header 0x35352c32 at offset 96960.\nNote: Trying to resync...\nNote: Hit end of (available) data during resync.\nNote: Illegal Audio-MPEG-Header 0x32383337 at offset 108480.\nNote: Trying to resync...\nNote: Hit end of (available) data during resync.\nNote: Illegal Audio-MPEG-Header 0x392c3239 at offset 84480.\nNote: Trying to resync...\nNote: Hit end of (available) data during resync.\nNote: Illegal Audio-MPEG-Header 0x3cb8fb92 at offset 95040.\nNote: Trying to resync...\nNote: Hit end of (available) data during resync.\nNote: Illegal Audio-MPEG-Header 0x2c323931 at offset 99840.\nNote: Trying to resync...\nNote: Skipped 159 bytes in input.\nNote: Illegal Audio-MPEG-Header 0x322c3238 at offset 122880.\nNote: Trying to resync...\nNote: Hit end of (available) data during resync.\nNote: Illegal Audio-MPEG-Header 0x37393236 at offset 108480.\nNote: Trying to resync...\nNote: Hit end of (available) data during resync.\nNote: Illegal Audio-MPEG-Header 0x38342c31 at offset 101760.\nNote: Trying to resync...\nNote: Hit end of (available) data during resync.\nNote: Illegal Audio-MPEG-Header 0x3337342c at offset 132480.\nNote: Trying to resync...\nNote: Hit end of (available) data during resync.\nNote: Illegal Audio-MPEG-Header 0x38363731 at offset 87360.\nNote: Trying to resync...\nNote: Skipped 242 bytes in input.\nNote: Illegal Audio-MPEG-Header 0x322c3138 at offset 133440.\nNote: Trying to resync...\nNote: Hit end of (available) data during resync.\nNote: Illegal Audio-MPEG-Header 0x372c3130 at offset 101760.\nNote: Trying to resync...\nNote: Skipped 105 bytes in input.\nNote: Illegal Audio-MPEG-Header 0x2c323932 at offset 114240.\nNote: Trying to resync...\nNote: Hit end of (available) data during resync.\n","output_type":"stream"},{"output_type":"display_data","data":{"text/plain":"Map:   0%|          | 0/61 [00:00<?, ? examples/s]","application/vnd.jupyter.widget-view+json":{"version_major":2,"version_minor":0,"model_id":"91f51d3eb7fe4aff939de2d7d8d681d5"}},"metadata":{}},{"name":"stderr","text":"Note: Illegal Audio-MPEG-Header 0x38342c38 at offset 60480.\nNote: Trying to resync...\nNote: Hit end of (available) data during resync.\nNote: Illegal Audio-MPEG-Header 0x7fe60bb2 at offset 79680.\nNote: Trying to resync...\nNote: Hit end of (available) data during resync.\nNote: Illegal Audio-MPEG-Header 0x614e4daa at offset 77760.\nNote: Trying to resync...\nNote: Hit end of (available) data during resync.\nNote: Illegal Audio-MPEG-Header 0x3038332c at offset 135360.\nNote: Trying to resync...\nNote: Skipped 357 bytes in input.\n","output_type":"stream"},{"output_type":"display_data","data":{"text/plain":"Map:   0%|          | 0/61 [00:00<?, ? examples/s]","application/vnd.jupyter.widget-view+json":{"version_major":2,"version_minor":0,"model_id":"d826a9ffa2054f5c8a85694f5c8a720a"}},"metadata":{}},{"name":"stderr","text":"Note: Illegal Audio-MPEG-Header 0x34342c32 at offset 80640.\nNote: Trying to resync...\nNote: Skipped 217 bytes in input.\nNote: Illegal Audio-MPEG-Header 0xc719dc7b at offset 99840.\nNote: Trying to resync...\nNote: Hit end of (available) data during resync.\nNote: Illegal Audio-MPEG-Header 0xb059ebe7 at offset 127680.\nNote: Trying to resync...\nNote: Hit end of (available) data during resync.\nNote: Illegal Audio-MPEG-Header 0x7cd47323 at offset 111360.\nNote: Trying to resync...\nNote: Hit end of (available) data during resync.\nNote: Illegal Audio-MPEG-Header 0x342c3239 at offset 114240.\nNote: Trying to resync...\nNote: Hit end of (available) data during resync.\n","output_type":"stream"},{"name":"stdout","text":"✅ Dataset ready!\n\n","output_type":"stream"}],"execution_count":29},{"cell_type":"markdown","source":"# data collateral","metadata":{}},{"cell_type":"code","source":"from dataclasses import dataclass\nfrom typing import Any, Dict, List, Union\nimport torch\n\n@dataclass\nclass DataCollatorSpeechSeq2SeqWithPadding:\n    processor: Any\n    decoder_start_token_id: int\n\n    def __call__(self, features: List[Dict[str, Union[List[int], torch.Tensor]]]) -> Dict[str, torch.Tensor]:\n        # Extract input features from batch\n        input_features = [{\"input_features\": feature[\"input_features\"]} for feature in features]\n        batch = self.processor.feature_extractor.pad(input_features, return_tensors=\"pt\")\n        \n        # Extract labels\n        label_features = [{\"input_ids\": feature[\"labels\"]} for feature in features]\n        labels_batch = self.processor.tokenizer.pad(label_features, return_tensors=\"pt\")\n        \n        # Replace padding with -100 to ignore loss correctly\n        labels = labels_batch[\"input_ids\"].masked_fill(labels_batch.attention_mask.ne(1), -100)\n        \n        # If bos token is prepended in previous tokenization step, remove it\n        if (labels[:, 0] == self.decoder_start_token_id).all().cpu().item():\n            labels = labels[:, 1:]\n        \n        batch[\"labels\"] = labels\n        \n        return batch  # ✅ Return the full batch dict\n\ndata_collator = DataCollatorSpeechSeq2SeqWithPadding(\n    processor=processor,\n    decoder_start_token_id=model.config.decoder_start_token_id,\n)\n","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-11-06T16:30:21.663414Z","iopub.execute_input":"2025-11-06T16:30:21.663630Z","iopub.status.idle":"2025-11-06T16:30:21.670905Z","shell.execute_reply.started":"2025-11-06T16:30:21.663616Z","shell.execute_reply":"2025-11-06T16:30:21.670340Z"}},"outputs":[],"execution_count":30},{"cell_type":"markdown","source":"# evaluation metrics","metadata":{}},{"cell_type":"code","source":"# Metric computation\nmetric = evaluate.load(\"wer\")\n\ndef compute_metrics(pred):\n    pred_ids = pred.predictions\n    label_ids = pred.label_ids\n\n    label_ids[label_ids == -100] = tokenizer.pad_token_id\n\n    pred_str = tokenizer.batch_decode(pred_ids, skip_special_tokens=True)\n    label_str = tokenizer.batch_decode(label_ids, skip_special_tokens=True)\n\n    wer = 100 * metric.compute(predictions=pred_str, references=label_str)\n\n    return {\"wer\": wer}","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-11-06T16:30:21.672343Z","iopub.execute_input":"2025-11-06T16:30:21.672646Z","iopub.status.idle":"2025-11-06T16:30:23.273658Z","shell.execute_reply.started":"2025-11-06T16:30:21.672630Z","shell.execute_reply":"2025-11-06T16:30:23.273056Z"}},"outputs":[{"output_type":"display_data","data":{"text/plain":"Downloading builder script: 0.00B [00:00, ?B/s]","application/vnd.jupyter.widget-view+json":{"version_major":2,"version_minor":0,"model_id":"f7c0937afaf3418894342fcd7cfff794"}},"metadata":{}}],"execution_count":31},{"cell_type":"markdown","source":"# Training Configuration (OPTIMIZED FOR LORA)\n","metadata":{}},{"cell_type":"code","source":"\n# Enable gradient checkpointing for memory efficiency\nmodel.gradient_checkpointing_enable()\nmodel.config.use_cache = False\n\n# Calculate steps per epoch\nsteps_per_epoch = len(train_dataset) // (4 * 4)  # batch_size * gradient_accumulation_steps\nprint(f\"📊 Training Configuration:\")\nprint(f\"   Steps per epoch: {steps_per_epoch}\")\nprint(f\"   Max epochs: 100\")\nprint(f\"   Early stopping patience: 10 epochs\")\nprint(f\"   Evaluation: Every epoch\\n\")\n\n\n# Training arguments\ntraining_args = Seq2SeqTrainingArguments(\n    output_dir=\"./whisper-finetuned-lora-epochs\",\n    \n    # ✅ EPOCH-BASED TRAINING\n    num_train_epochs=100,              # Train for up to 100 epochs\n    \n    # Batch configuration\n    per_device_train_batch_size=4,\n    per_device_eval_batch_size=4,\n    gradient_accumulation_steps=4,     # Effective batch size = 16\n    \n    # Learning rate\n    learning_rate=1e-3,\n    warmup_ratio=0.1,                  # 10% of training for warmup\n    \n    # ✅ EVALUATION EVERY EPOCH\n    eval_strategy=\"epoch\",             # Evaluate after each epoch\n    save_strategy=\"epoch\",             # Save after each epoch\n    \n    # Model selection\n    load_best_model_at_end=True,       # Load best model at end\n    metric_for_best_model=\"wer\",       # Use WER as metric\n    greater_is_better=False,           # Lower WER is better\n    \n    # Generation settings\n    predict_with_generate=True,\n    generation_max_length=225,\n    \n    # Logging\n    logging_strategy=\"steps\",\n    logging_steps=10,\n    report_to=[\"tensorboard\"],\n    \n    # Memory optimization\n    gradient_checkpointing=True,\n    fp16=True,\n    \n    # Other settings\n    push_to_hub=False,\n    remove_unused_columns=False,\n    dataloader_num_workers=2,\n    save_total_limit=3,                # Keep only 3 best checkpoints\n)\n\n# ✅ ADD EARLY STOPPING CALLBACK\nearly_stopping = EarlyStoppingCallback(\n    early_stopping_patience=10,        # Stop if no improvement for 10 epochs\n    early_stopping_threshold=0.01      # Minimum improvement threshold (0.01 WER)\n)\n","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-11-06T16:31:37.058785Z","iopub.execute_input":"2025-11-06T16:31:37.059043Z","iopub.status.idle":"2025-11-06T16:31:37.105842Z","shell.execute_reply.started":"2025-11-06T16:31:37.059028Z","shell.execute_reply":"2025-11-06T16:31:37.105173Z"}},"outputs":[{"name":"stdout","text":"📊 Training Configuration:\n   Steps per epoch: 30\n   Max epochs: 100\n   Early stopping patience: 10 epochs\n   Evaluation: Every epoch\n\n","output_type":"stream"}],"execution_count":33},{"cell_type":"markdown","source":"# Initialize trainer","metadata":{}},{"cell_type":"code","source":"# Verify your dataset structure\nprint(\"Checking dataset structure:\")\nprint(\"Train sample keys:\", train_dataset[0].keys())\nprint(\"Input features shape:\", np.array(train_dataset[0][\"input_features\"]).shape)\nprint(\"Labels sample:\", train_dataset[0][\"labels\"][:10])\n\n# Test data collator\nprint(\"\\nTesting data collator:\")\nsample_batch = [train_dataset[0], train_dataset[1]]\n\ncollated = data_collator(sample_batch)\nprint(\"Collated batch keys:\", collated.keys())\nprint(\"Input features shape:\", collated[\"input_features\"].shape)\nprint(\"Labels shape:\", collated[\"labels\"].shape)\n\n# ✅ CRITICAL FIX: Pass feature_extractor as tokenizer\ntrainer = Seq2SeqTrainer(\n    args=training_args,\n    model=model,\n    train_dataset=train_dataset,\n    eval_dataset=val_dataset,\n    data_collator=data_collator,\n    compute_metrics=compute_metrics,\n    tokenizer=processor.feature_extractor,  # ✅ Use feature_extractor, NOT tokenizer\n    callbacks=[early_stopping], \n)\n\nprint(\"🚀 Starting LoRA training...\\n\")\nprint(\"=\"*60)\n\n# Clear GPU cache before training\ntorch.cuda.empty_cache()\n\ntrainer.train()\n\nprint(\"\\n\" + \"=\"*60)\nprint(\"✅ Training complete!\")\nprint(\"=\"*60)","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-11-06T16:32:18.924639Z","iopub.execute_input":"2025-11-06T16:32:18.925203Z","execution_failed":"2025-11-06T16:34:37.072Z"}},"outputs":[{"name":"stdout","text":"Checking dataset structure:\nTrain sample keys: dict_keys(['input_features', 'labels'])\nInput features shape: (128, 3000)\nLabels sample: [50258, 50259, 50360, 50364, 35, 595, 9598, 13083, 281, 452]\n\nTesting data collator:\nCollated batch keys: KeysView({'input_features': tensor([[[-0.5710, -0.5710, -0.5710,  ..., -0.5710, -0.5710, -0.5710],\n         [-0.5710, -0.5710, -0.5710,  ..., -0.5710, -0.5710, -0.5710],\n         [-0.5710, -0.5710, -0.5710,  ..., -0.5710, -0.5710, -0.5710],\n         ...,\n         [-0.5710, -0.5710, -0.5710,  ..., -0.5710, -0.5710, -0.5710],\n         [-0.5710, -0.5710, -0.5710,  ..., -0.5710, -0.5710, -0.5710],\n         [-0.5710, -0.5710, -0.5710,  ..., -0.5710, -0.5710, -0.5710]],\n\n        [[ 0.4353,  0.4215,  0.3846,  ..., -0.6594, -0.6594, -0.6594],\n         [ 0.5329,  0.5191,  0.4822,  ..., -0.6594, -0.6594, -0.6594],\n         [-0.0737,  0.3685,  0.3590,  ..., -0.6594, -0.6594, -0.6594],\n         ...,\n         [-0.6594, -0.6594, -0.6594,  ..., -0.6594, -0.6594, -0.6594],\n         [-0.6594, -0.6594, -0.6594,  ..., -0.6594, -0.6594, -0.6594],\n         [-0.6594, -0.6594, -0.6594,  ..., -0.6594, -0.6594, -0.6594]]]), 'labels': tensor([[50259, 50360, 50364,    35,   595,  9598, 13083,   281,   452,  2696,\n         50257,  -100,  -100,  -100],\n        [50259, 50360, 50364,    40,   643,   281,   909,  6905,  4875,    70,\n           270,   281, 16599, 50257]])})\nInput features shape: torch.Size([2, 128, 3000])\nLabels shape: torch.Size([2, 14])\n","output_type":"stream"},{"name":"stderr","text":"No label_names provided for model class `PeftModel`. Since `PeftModel` hides base models input arguments, if label_names is not given, label_names can't be set automatically within `Trainer`. Note that empty label_names list will be used instead.\n","output_type":"stream"},{"name":"stdout","text":"🚀 Starting LoRA training...\n\n============================================================\n","output_type":"stream"},{"output_type":"display_data","data":{"text/plain":"<IPython.core.display.HTML object>","text/html":"\n    <div>\n      \n      <progress value='13' max='3100' style='width:300px; height:20px; vertical-align: middle;'></progress>\n      [  13/3100 01:57 < 9:07:31, 0.09 it/s, Epoch 0.39/100]\n    </div>\n    <table border=\"1\" class=\"dataframe\">\n  <thead>\n <tr style=\"text-align: left;\">\n      <th>Epoch</th>\n      <th>Training Loss</th>\n      <th>Validation Loss</th>\n    </tr>\n  </thead>\n  <tbody>\n  </tbody>\n</table><p>"},"metadata":{}}],"execution_count":null},{"cell_type":"markdown","source":"# Visulizing Training history","metadata":{}},{"cell_type":"code","source":"print(\"\\n📈 Creating training visualizations...\")\n\n# Extract training history\nlog_history = trainer.state.log_history\n\n# Parse logs\ntrain_loss = []\neval_loss = []\neval_wer = []\nepochs = []\n\nfor log in log_history:\n    if 'loss' in log and 'epoch' in log:\n        train_loss.append((log['epoch'], log['loss']))\n    if 'eval_loss' in log:\n        eval_loss.append((log['epoch'], log['eval_loss']))\n        eval_wer.append((log['epoch'], log['eval_wer']))\n        epochs.append(log['epoch'])\n\n# Create visualization\nfig, axes = plt.subplots(2, 2, figsize=(15, 10))\n\n# Plot 1: Training Loss\nif train_loss:\n    epochs_train, losses = zip(*train_loss)\n    axes[0, 0].plot(epochs_train, losses, 'b-', linewidth=2)\n    axes[0, 0].set_title('Training Loss Over Time', fontsize=14, fontweight='bold')\n    axes[0, 0].set_xlabel('Epoch')\n    axes[0, 0].set_ylabel('Loss')\n    axes[0, 0].grid(True, alpha=0.3)\n\n# Plot 2: Validation Loss\nif eval_loss:\n    epochs_val, losses_val = zip(*eval_loss)\n    axes[0, 1].plot(epochs_val, losses_val, 'r-', linewidth=2)\n    axes[0, 1].set_title('Validation Loss', fontsize=14, fontweight='bold')\n    axes[0, 1].set_xlabel('Epoch')\n    axes[0, 1].set_ylabel('Loss')\n    axes[0, 1].grid(True, alpha=0.3)\n\n# Plot 3: Word Error Rate (WER)\nif eval_wer:\n    epochs_wer, wer_vals = zip(*eval_wer)\n    axes[1, 0].plot(epochs_wer, wer_vals, 'g-', linewidth=2, marker='o')\n    best_wer_idx = np.argmin(wer_vals)\n    axes[1, 0].axvline(x=epochs_wer[best_wer_idx], color='r', linestyle='--', \n                       label=f'Best WER: {wer_vals[best_wer_idx]:.2f}% at epoch {epochs_wer[best_wer_idx]:.0f}')\n    axes[1, 0].set_title('Word Error Rate (WER)', fontsize=14, fontweight='bold')\n    axes[1, 0].set_xlabel('Epoch')\n    axes[1, 0].set_ylabel('WER (%)')\n    axes[1, 0].legend()\n    axes[1, 0].grid(True, alpha=0.3)\n\n# Plot 4: Training vs Validation Loss\nif train_loss and eval_loss:\n    axes[1, 1].plot(epochs_train, losses, 'b-', label='Train Loss', linewidth=2)\n    axes[1, 1].plot(epochs_val, losses_val, 'r-', label='Val Loss', linewidth=2)\n    axes[1, 1].set_title('Training vs Validation Loss', fontsize=14, fontweight='bold')\n    axes[1, 1].set_xlabel('Epoch')\n    axes[1, 1].set_ylabel('Loss')\n    axes[1, 1].legend()\n    axes[1, 1].grid(True, alpha=0.3)\n\nplt.tight_layout()\nplt.savefig('./training_history.png', dpi=300, bbox_inches='tight')\nprint(\"✅ Saved training_history.png\")\n","metadata":{"trusted":true},"outputs":[],"execution_count":null},{"cell_type":"markdown","source":"# evaluation on test ","metadata":{}},{"cell_type":"code","source":"# EVALUATE BEST MODEL ON TEST SET\n# ============================================================================\n\nprint(\"\\n🧪 Evaluating best model on test set...\")\ntest_results = trainer.evaluate(test_dataset)\n\nprint(f\"\\n{'='*60}\")\nprint(f\"📊 FINAL TEST RESULTS:\")\nprint(f\"   Test WER: {test_results['eval_wer']:.2f}%\")\nprint(f\"   Test Loss: {test_results['eval_loss']:.4f}\")\nprint(f\"{'='*60}\\n\")\n","metadata":{"trusted":true},"outputs":[],"execution_count":null},{"cell_type":"code","source":"# Save LoRA adapters only\nmodel.save_pretrained(\"./whisper-lora-adapters-best\")\nprocessor.save_pretrained(\"./whisper-lora-adapters-best\")\nprint(\"✅ LoRA adapters saved!\")\n\n# Merge and save full model\nprint(\"🔀 Merging LoRA weights...\")\nmerged_model = model.merge_and_unload()\nmerged_model.save_pretrained(\"./whisper-malaysian-final\")\nprocessor.save_pretrained(\"./whisper-malaysian-final\")\nprint(\"✅ Merged model saved!\")\n","metadata":{"trusted":true},"outputs":[],"execution_count":null},{"cell_type":"markdown","source":"","metadata":{}},{"cell_type":"code","source":"# QUICK INFERENCE TEST\n\nprint(\"\\n🎯 Testing inference on random samples...\\n\")\n\nfrom transformers import pipeline\n\npipe = pipeline(\n    \"automatic-speech-recognition\",\n    model=\"./whisper-malaysian-final\",\n    tokenizer=processor.tokenizer,\n    feature_extractor=processor.feature_extractor,\n    device=0 if torch.cuda.is_available() else -1\n)\n\ntest_samples = test_df.sample(min(5, len(test_df)))\n\nfor idx, row in test_samples.iterrows():\n    prediction = pipe(row['audio_path'])[\"text\"]\n    print(f\"📁 File: {row['filename']}\")\n    print(f\"✅ Expected:  {row['transcription']}\")\n    print(f\"🎤 Predicted: {prediction}\")\n    print(f\"-\" * 60)\n","metadata":{"trusted":true},"outputs":[],"execution_count":null},{"cell_type":"code","source":"print(f\"\\n{'='*60}\")\nprint(\"🎉 TRAINING COMPLETE!\")\nprint(f\"{'='*60}\")\nprint(f\"📊 Training Summary:\")\nprint(f\"   Total epochs trained: {int(trainer.state.epoch)}\")\nprint(f\"   Best epoch: {trainer.state.best_model_checkpoint.split('-')[-1] if trainer.state.best_model_checkpoint else 'N/A'}\")\nprint(f\"   Best validation WER: {trainer.state.best_metric:.2f}%\")\nprint(f\"   Final test WER: {test_results['eval_wer']:.2f}%\")\nprint(f\"\\n📦 Saved Models:\")\nprint(f\"   1. LoRA adapters: ./whisper-lora-adapters-best (~10-50MB)\")\nprint(f\"   2. Full model: ./whisper-malaysian-final (~3GB)\")\nprint(f\"   3. Training plots: ./training_history.png\")\nprint(f\"\\n💡 To load the model later:\")\nprint(f\"   model = WhisperForConditionalGeneration.from_pretrained('./whisper-malaysian-final')\")\nprint(f\"{'='*60}\\n\")\n\n","metadata":{"trusted":true},"outputs":[],"execution_count":null}]}
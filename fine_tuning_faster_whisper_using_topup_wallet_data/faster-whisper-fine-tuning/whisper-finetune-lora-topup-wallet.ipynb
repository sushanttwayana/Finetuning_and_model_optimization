{"metadata":{"kernelspec":{"language":"python","display_name":"Python 3","name":"python3"},"language_info":{"name":"python","version":"3.11.13","mimetype":"text/x-python","codemirror_mode":{"name":"ipython","version":3},"pygments_lexer":"ipython3","nbconvert_exporter":"python","file_extension":".py"},"kaggle":{"accelerator":"gpu","dataSources":[{"sourceId":13620630,"sourceType":"datasetVersion","datasetId":8646220}],"isInternetEnabled":true,"language":"python","sourceType":"notebook","isGpuEnabled":true}},"nbformat_minor":4,"nbformat":4,"cells":[{"cell_type":"code","source":"!apt-get update -qq","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-11-06T09:31:24.068437Z","iopub.execute_input":"2025-11-06T09:31:24.068705Z","iopub.status.idle":"2025-11-06T09:31:27.001485Z","shell.execute_reply.started":"2025-11-06T09:31:24.068676Z","shell.execute_reply":"2025-11-06T09:31:27.000809Z"}},"outputs":[],"execution_count":null},{"cell_type":"code","source":"!apt-get install -y ffmpeg libsndfile1","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-11-06T09:31:34.555000Z","iopub.execute_input":"2025-11-06T09:31:34.555273Z","iopub.status.idle":"2025-11-06T09:31:36.848463Z","shell.execute_reply.started":"2025-11-06T09:31:34.555253Z","shell.execute_reply":"2025-11-06T09:31:36.847758Z"}},"outputs":[],"execution_count":null},{"cell_type":"code","source":"!pip install --upgrade pip","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-11-06T09:31:36.849881Z","iopub.execute_input":"2025-11-06T09:31:36.850121Z","iopub.status.idle":"2025-11-06T09:31:42.612166Z","shell.execute_reply.started":"2025-11-06T09:31:36.850101Z","shell.execute_reply":"2025-11-06T09:31:42.611418Z"}},"outputs":[],"execution_count":null},{"cell_type":"code","source":"!pip install pydantic==2.11.0 rich==13.7.1 pyarrow==19.0.0 --force-reinstall","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-11-06T09:31:42.613116Z","iopub.execute_input":"2025-11-06T09:31:42.613352Z","iopub.status.idle":"2025-11-06T09:32:03.230074Z","shell.execute_reply.started":"2025-11-06T09:31:42.613331Z","shell.execute_reply":"2025-11-06T09:32:03.229182Z"}},"outputs":[],"execution_count":null},{"cell_type":"code","source":"!pip install transformers datasets accelerate","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-11-06T09:32:03.232160Z","iopub.execute_input":"2025-11-06T09:32:03.232448Z","iopub.status.idle":"2025-11-06T09:33:18.090467Z","shell.execute_reply.started":"2025-11-06T09:32:03.232428Z","shell.execute_reply":"2025-11-06T09:33:18.089773Z"}},"outputs":[],"execution_count":null},{"cell_type":"code","source":"!pip install audiomentations==0.36.0","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-11-06T09:33:18.091339Z","iopub.execute_input":"2025-11-06T09:33:18.091564Z","iopub.status.idle":"2025-11-06T09:33:30.333311Z","shell.execute_reply.started":"2025-11-06T09:33:18.091543Z","shell.execute_reply":"2025-11-06T09:33:30.332613Z"}},"outputs":[],"execution_count":null},{"cell_type":"code","source":"!pip install  tensorboard numpy scipy datasets ","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-11-06T09:33:30.334413Z","iopub.execute_input":"2025-11-06T09:33:30.334717Z","iopub.status.idle":"2025-11-06T09:33:32.341177Z","shell.execute_reply.started":"2025-11-06T09:33:30.334685Z","shell.execute_reply":"2025-11-06T09:33:32.340447Z"}},"outputs":[],"execution_count":null},{"cell_type":"code","source":"!pip install peft>=0.12.0 bitsandbytes>=0.43.3","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-11-06T09:33:32.342231Z","iopub.execute_input":"2025-11-06T09:33:32.342536Z","iopub.status.idle":"2025-11-06T09:33:36.871665Z","shell.execute_reply.started":"2025-11-06T09:33:32.342506Z","shell.execute_reply":"2025-11-06T09:33:36.870622Z"}},"outputs":[],"execution_count":null},{"cell_type":"code","source":"!pip install tqdm==4.67.1 scikit-learn==1.2.2","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-11-06T09:33:36.872833Z","iopub.execute_input":"2025-11-06T09:33:36.873127Z","iopub.status.idle":"2025-11-06T09:33:38.810121Z","shell.execute_reply.started":"2025-11-06T09:33:36.873093Z","shell.execute_reply":"2025-11-06T09:33:38.809196Z"}},"outputs":[],"execution_count":null},{"cell_type":"code","source":"pip install evaluate jiwer","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-11-06T09:33:38.812516Z","iopub.execute_input":"2025-11-06T09:33:38.812756Z","iopub.status.idle":"2025-11-06T09:33:42.766847Z","shell.execute_reply.started":"2025-11-06T09:33:38.812736Z","shell.execute_reply":"2025-11-06T09:33:42.766133Z"}},"outputs":[],"execution_count":null},{"cell_type":"code","source":"# pip list","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-11-06T09:33:42.767696Z","iopub.execute_input":"2025-11-06T09:33:42.767940Z","iopub.status.idle":"2025-11-06T09:33:42.771479Z","shell.execute_reply.started":"2025-11-06T09:33:42.767915Z","shell.execute_reply":"2025-11-06T09:33:42.770953Z"}},"outputs":[],"execution_count":null},{"cell_type":"code","source":"import datasets\nprint(datasets.__version__)","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-11-06T09:33:42.772199Z","iopub.execute_input":"2025-11-06T09:33:42.772434Z","iopub.status.idle":"2025-11-06T09:33:44.143464Z","shell.execute_reply.started":"2025-11-06T09:33:42.772411Z","shell.execute_reply":"2025-11-06T09:33:44.142849Z"}},"outputs":[],"execution_count":null},{"cell_type":"code","source":"import os\nimport re\nimport json\nimport torch\nimport warnings\nwarnings.filterwarnings('ignore')\n\nimport numpy as np\nimport pandas as pd\nfrom pathlib import Path\nfrom dataclasses import dataclass\nfrom typing import Dict, List, Union, Any\nfrom tqdm.auto import tqdm\n\nimport librosa\nimport soundfile as sf\nfrom audiomentations import Compose, AddGaussianNoise, TimeStretch, PitchShift, Shift\n\nfrom datasets import Dataset, DatasetDict, Audio\nfrom transformers import (\n    WhisperFeatureExtractor,\n    WhisperTokenizer, \n    WhisperProcessor,\n    WhisperForConditionalGeneration,\n    Seq2SeqTrainingArguments,\n    Seq2SeqTrainer\n)\nimport evaluate\nfrom transformers import BitsAndBytesConfig\nfrom peft import LoraConfig, get_peft_model, prepare_model_for_kbit_training\n\n# GPU Check\nprint(f\"\\n{'='*60}\")\nprint(f\"üñ•Ô∏è  GPU Available: {torch.cuda.is_available()}\")\nif torch.cuda.is_available():\n    print(f\"üìä GPU: {torch.cuda.get_device_name(0)}\")\n    print(f\"üíæ GPU Memory: {torch.cuda.get_device_properties(0).total_memory / 1e9:.2f} GB\")\nprint(f\"{'='*60}\\n\")\n\n","metadata":{"_uuid":"8f2839f25d086af736a60e9eeb907d3b93b6e0e5","_cell_guid":"b1076dfc-b9ad-4769-8c92-a6c4dae69d19","trusted":true,"execution":{"iopub.status.busy":"2025-11-06T09:33:44.144254Z","iopub.execute_input":"2025-11-06T09:33:44.144687Z","iopub.status.idle":"2025-11-06T09:34:07.815111Z","shell.execute_reply.started":"2025-11-06T09:33:44.144638Z","shell.execute_reply":"2025-11-06T09:34:07.814182Z"}},"outputs":[],"execution_count":null},{"cell_type":"markdown","source":"# Dataset Loading and prasing","metadata":{}},{"cell_type":"markdown","source":"### Chunking long audio","metadata":{}},{"cell_type":"code","source":"def chunk_long_audio(audio_path, transcription, max_duration=30.0, overlap=1.0):\n    \"\"\"\n    Split long audio files into chunks of max_duration seconds.\n    \n    Args:\n        audio_path: Path to audio file\n        transcription: Full transcription text\n        max_duration: Maximum chunk duration in seconds (default 30s for Whisper)\n        overlap: Overlap between chunks in seconds to avoid cutting words\n    \n    Returns:\n        List of dicts with chunked audio paths and estimated transcriptions\n    \"\"\"\n    audio, sr = librosa.load(audio_path, sr=16000)\n    audio_duration = len(audio) / sr\n    \n    # If audio is short enough, return as-is\n    if audio_duration <= max_duration:\n        return [{'audio_path': audio_path, 'transcription': transcription}]\n    \n    # Calculate chunk parameters\n    chunk_samples = int(max_duration * sr)\n    overlap_samples = int(overlap * sr)\n    step_samples = chunk_samples - overlap_samples\n    \n    chunks = []\n    words = transcription.split()\n    total_chunks = int(np.ceil((len(audio) - chunk_samples) / step_samples)) + 1\n    words_per_chunk = max(1, len(words) // total_chunks)\n    \n    chunk_idx = 0\n    word_start = 0\n    \n    for start in range(0, len(audio) - overlap_samples, step_samples):\n        end = min(start + chunk_samples, len(audio))\n        chunk_audio = audio[start:end]\n        \n        # Estimate text for this chunk (proportional split)\n        word_end = min(word_start + words_per_chunk, len(words))\n        \n        # For last chunk, take remaining words\n        if start + step_samples >= len(audio) - chunk_samples:\n            word_end = len(words)\n        \n        chunk_text = ' '.join(words[word_start:word_end])\n        \n        # Save chunk temporarily\n        chunk_filename = f\"chunk_{chunk_idx}_{Path(audio_path).name}\"\n        chunk_path = f\"/kaggle/working/chunks/{chunk_filename}\"\n        os.makedirs(\"/kaggle/working/chunks\", exist_ok=True)\n        sf.write(chunk_path, chunk_audio, sr)\n        \n        chunks.append({\n            'audio_path': chunk_path,\n            'transcription': chunk_text.strip()\n        })\n        \n        word_start = word_end\n        chunk_idx += 1\n        \n        # Stop if we've processed all audio\n        if end >= len(audio):\n            break\n    \n    print(f\"  üìå Split {Path(audio_path).name} ({audio_duration:.1f}s) ‚Üí {len(chunks)} chunks\")\n    return chunks","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-11-06T09:34:07.816347Z","iopub.execute_input":"2025-11-06T09:34:07.817344Z","iopub.status.idle":"2025-11-06T09:34:07.826623Z","shell.execute_reply.started":"2025-11-06T09:34:07.817309Z","shell.execute_reply":"2025-11-06T09:34:07.825775Z"}},"outputs":[],"execution_count":null},{"cell_type":"code","source":"def augment_audio(audio, sr):\n    augmenter = Compose([\n        AddGaussianNoise(min_amplitude=0.001, max_amplitude=0.015, p=0.5),\n        TimeStretch(min_rate=0.9, max_rate=1.1, p=0.5),\n        PitchShift(min_semitones=-2, max_semitones=2, p=0.5),\n        Shift(min_shift=-0.5, max_shift=0.5, p=0.3),\n    ])\n    \n    return augmenter(samples=audio, sample_rate=sr)","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-11-06T09:34:07.827501Z","iopub.execute_input":"2025-11-06T09:34:07.827827Z","iopub.status.idle":"2025-11-06T09:34:07.855835Z","shell.execute_reply.started":"2025-11-06T09:34:07.827802Z","shell.execute_reply":"2025-11-06T09:34:07.855115Z"}},"outputs":[],"execution_count":null},{"cell_type":"code","source":"def parse_text_file(text_path):\n    \"\"\"\n    Parse text files handling both formats:\n    - '1.Text here' or '1. Text here' ‚Üí removes number prefix\n    - 'Text here' ‚Üí uses as-is\n    Returns: list of (line_number, cleaned_text) tuples\n    \"\"\"\n    with open(text_path, 'r', encoding='utf-8') as f:\n        lines = f.readlines()\n    \n    parsed = []\n    for line in lines:\n        original = line.strip()\n        if not original:\n            continue\n            \n        # Extract line number if present: \"1.\" or \"100.\" etc\n        match = re.match(r'^(\\d+)\\.?\\s*(.+)$', original)\n        if match:\n            line_num = int(match.group(1))\n            text = match.group(2).strip()\n        else:\n            line_num = None\n            text = original\n        \n        parsed.append((line_num, text))\n    \n    return parsed\n\n\ndef extract_audio_number(filename):\n    \"\"\"\n    Extract number from various audio filename formats:\n    - 'Voice 1.mp3' ‚Üí 1\n    - '1_load my wallet.mp3' ‚Üí 1\n    - 'Standard recording 1.mp3' ‚Üí 1\n    - 'recording_001.mp3' ‚Üí 1\n    Returns: number or None\n    \"\"\"\n    # Try pattern: '1_text.mp3'\n    match = re.match(r'^(\\d+)_', filename)\n    if match:\n        return int(match.group(1))\n    \n    # Try pattern: 'text 1.mp3' or 'text_1.mp3'\n    match = re.search(r'[\\s_](\\d+)\\.mp3$', filename)\n    if match:\n        return int(match.group(1))\n    \n    # Try pattern: 'text001.mp3'\n    match = re.search(r'(\\d+)\\.mp3$', filename)\n    if match:\n        return int(match.group(1))\n    \n    return None\n\n\ndef create_dataset_manifest(base_path):\n    \"\"\"\n    Create audio-text pairs with intelligent matching.\n    Handles both numbered and sequential matching.\n    \"\"\"\n    \n    dataset_entries = []\n    \n    # Mapping: audio_folder ‚Üí text_file\n    mappings = {\n        'Voice_memo': 'voice_text_dataset/voice_text_dataset/voice_memo_text_data.txt',\n        'chinese_accent': 'voice_text_dataset/voice_text_dataset/chinese_accent_text_data.txt',\n        'voice_record': 'voice_text_dataset/voice_text_dataset/voice_record_text_data.txt',\n        # 'my_voice': 'voice_text_dataset/voice_text_dataset/my_voice_text_data.txt'\n    }\n    \n    total_mismatches = 0\n    \n    for audio_dir, text_file in mappings.items():\n        audio_path = Path(base_path) / audio_dir\n        text_path = Path(base_path) / text_file\n        \n        if not audio_path.exists():\n            print(f\"‚ö†Ô∏è  Skipping: {audio_dir} (not found)\")\n            continue\n        \n        if not text_path.exists():\n            print(f\"‚ö†Ô∏è  Skipping: {text_file} (not found)\")\n            continue\n        \n        # Get audio files\n        audio_files = sorted(audio_path.glob('*.mp3'))\n        \n        # Parse transcriptions\n        transcriptions = parse_text_file(text_path)\n        \n        # Create mapping: number ‚Üí text\n        text_dict = {}\n        for line_num, text in transcriptions:\n            if line_num:\n                text_dict[line_num] = text\n        \n        # Match audio files to transcriptions\n        matched = 0\n        for audio_file in audio_files:\n            # Try to extract number from filename\n            audio_num = extract_audio_number(audio_file.name)\n            \n            if audio_num and audio_num in text_dict:\n                # Number-based matching\n                transcription = text_dict[audio_num]\n                matched += 1\n            elif len(transcriptions) > 0:\n                # Sequential fallback (use first available)\n                line_num, transcription = transcriptions.pop(0)\n                matched += 1\n            else:\n                print(f\"‚ùå No transcription for: {audio_file.name}\")\n                total_mismatches += 1\n                continue\n            \n            dataset_entries.append({\n                'audio_path': str(audio_file),\n                'transcription': transcription,\n                'category': audio_dir,\n                'filename': audio_file.name\n            })\n        \n        print(f\"‚úÖ {audio_dir}: {matched} files matched\")\n    \n    # Handle single file with multi-sentence transcription\n    single_file = Path(base_path) / 'my_voice_sample.mp3'\n    single_text = Path(base_path) / 'voice_text_dataset/voice_text_dataset/my_voice_text_data.txt'\n    \n    if single_file.exists() and single_text.exists():\n        with open(single_text, 'r', encoding='utf-8') as f:\n            # Split by sentence if multiple exist\n            text = f.read().strip()\n            sentences = re.split(r'[.!?]+\\s+', text)\n            # Use full text as one entry\n            full_text = ' '.join(sentences).strip()\n            \n        dataset_entries.append({\n            'audio_path': str(single_file),\n            'transcription': full_text,\n            'category': 'single_voice',\n            'filename': single_file.name\n        })\n        print(f\"‚úÖ single_voice: 1 file matched\")\n    \n    print(f\"\\n{'='*60}\")\n    print(f\"üìä Total matched: {len(dataset_entries)} samples\")\n    if total_mismatches > 0:\n        print(f\"‚ö†Ô∏è  Mismatches: {total_mismatches} files\")\n    print(f\"{'='*60}\\n\")\n    \n    return dataset_entries\n\n\n# Create manifest\nBASE_PATH = '/kaggle/input/fine-tuning-dataset'\nprint(\"üîç Scanning dataset...\\n\")\ndataset_manifest = create_dataset_manifest(BASE_PATH)\n\n# Convert to DataFrame\ndf = pd.DataFrame(dataset_manifest)\nprint(\"\\nüìã Dataset Breakdown:\")\nprint(df.groupby('category').size())\nprint(f\"\\nüíæ Saving manifest...\")\ndf.to_csv('dataset_manifest.csv', index=False)\nprint(\"‚úÖ Saved to dataset_manifest.csv\")\n\n# Display sample\nprint(\"\\nüìù Sample entries:\")\nprint(df.head(3)[['filename', 'transcription', 'category']])\n","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-11-06T09:34:07.856582Z","iopub.execute_input":"2025-11-06T09:34:07.857400Z","iopub.status.idle":"2025-11-06T09:34:07.978954Z","shell.execute_reply.started":"2025-11-06T09:34:07.857374Z","shell.execute_reply":"2025-11-06T09:34:07.978120Z"}},"outputs":[],"execution_count":null},{"cell_type":"markdown","source":"# Audio Validation & Duration Calculation","metadata":{}},{"cell_type":"code","source":"import subprocess\n\ndef get_audio_duration(audio_path):\n    \"\"\"Get duration in seconds using FFmpeg\"\"\"\n    try:\n        result = subprocess.run(\n            ['ffprobe', '-v', 'error', '-show_entries', 'format=duration',\n             '-of', 'default=noprint_wrappers=1:nokey=1', audio_path],\n            stdout=subprocess.PIPE, stderr=subprocess.STDOUT, text=True\n        )\n        if result.returncode != 0:\n            print(f\"‚ùå FFmpeg error for {audio_path}: {result.stdout.strip()}\")\n            return 0\n        return float(result.stdout.strip())\n    except Exception as e:\n        print(f\"‚ùå Error processing {audio_path}: {e}\")\n        return 0\nprint(\"\\n‚è±Ô∏è  Calculating total duration...\")\ndf['duration'] = df['audio_path'].apply(get_audio_duration)\n\ntotal_duration_sec = df['duration'].sum()\ntotal_duration_min = total_duration_sec / 60\ntotal_duration_hr = total_duration_min / 60\n\nprint(f\"\\n{'='*60}\")\nprint(f\"üìä Dataset Statistics:\")\nprint(f\"   Total Samples: {len(df)}\")\nprint(f\"   Total Duration: {total_duration_min:.1f} minutes ({total_duration_hr:.2f} hours)\")\nprint(f\"   Avg Duration: {df['duration'].mean():.1f} seconds\")\nprint(f\"   Min Duration: {df['duration'].min():.1f} seconds\")\nprint(f\"   Max Duration: {df['duration'].max():.1f} seconds\")\nprint(f\"{'='*60}\\n\")\n\nif total_duration_hr < 0.5:\n    print(\"‚ö†Ô∏è  WARNING: Dataset < 0.5 hours\")\n    print(\"   Expected: Overfitting likely, limited generalization\")\n    print(\"   Recommendation: Collect 10-20 more hours for production use\")\n    print(\"   Proceeding with POC training...\\n\")\n","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-11-06T09:34:07.979833Z","iopub.execute_input":"2025-11-06T09:34:07.980206Z","iopub.status.idle":"2025-11-06T09:34:36.431380Z","shell.execute_reply.started":"2025-11-06T09:34:07.980179Z","shell.execute_reply":"2025-11-06T09:34:36.430767Z"}},"outputs":[],"execution_count":null},{"cell_type":"markdown","source":"# Data Augmentation (Critical for Small Datasets)","metadata":{}},{"cell_type":"code","source":"!apt-get update -qq && apt-get install -y ffmpeg","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-11-06T09:34:36.432026Z","iopub.execute_input":"2025-11-06T09:34:36.432208Z","iopub.status.idle":"2025-11-06T09:34:41.646864Z","shell.execute_reply.started":"2025-11-06T09:34:36.432193Z","shell.execute_reply":"2025-11-06T09:34:41.645970Z"}},"outputs":[],"execution_count":null},{"cell_type":"code","source":"def augment_audio(audio, sr):\n    \"\"\"\n    Apply audio augmentations to increase dataset diversity.\n    Helps combat overfitting on small datasets.\n    \"\"\"\n    augmenter = Compose([\n        AddGaussianNoise(min_amplitude=0.001, max_amplitude=0.015, p=0.5),\n        TimeStretch(min_rate=0.9, max_rate=1.1, p=0.5),\n        PitchShift(min_semitones=-2, max_semitones=2, p=0.5),\n        Shift(min_shift=-0.5, max_shift=0.5, p=0.3),\n    ])\n    \n    return augmenter(samples=audio, sample_rate=sr)\n\n\ndef create_augmented_dataset(df, augmentation_factor=2):\n    \"\"\"\n    Create augmented copies of dataset with audio chunking for long files.\n    \"\"\"\n    augmented_entries = []\n    \n    print(f\"üîÑ Creating {augmentation_factor}x augmented dataset with chunking...\")\n    \n    for idx, row in tqdm(df.iterrows(), total=len(df)):\n        # Check if audio needs chunking (>30s)\n        if row['duration'] > 30.0:\n            print(f\"\\n‚ö†Ô∏è  Long audio detected: {row['filename']} ({row['duration']:.1f}s)\")\n            chunks = chunk_long_audio(row['audio_path'], row['transcription'], max_duration=30.0)\n            \n            # Add all chunks\n            for chunk in chunks:\n                chunk_entry = row.to_dict()\n                chunk_entry['audio_path'] = chunk['audio_path']\n                chunk_entry['transcription'] = chunk['transcription']\n                chunk_entry['filename'] = Path(chunk['audio_path']).name\n                chunk_entry['duration'] = librosa.get_duration(path=chunk['audio_path'])\n                augmented_entries.append(chunk_entry)\n            \n            # Apply augmentation to chunks if needed\n            for aug_idx in range(augmentation_factor - 1):\n                for chunk in chunks:\n                    audio, sr = librosa.load(chunk['audio_path'], sr=16000)\n                    aug_audio = augment_audio(audio, sr)\n                    \n                    aug_filename = f\"aug_{aug_idx}_{Path(chunk['audio_path']).name}\"\n                    aug_path = f\"/kaggle/working/augmented/{aug_filename}\"\n                    os.makedirs(\"/kaggle/working/augmented\", exist_ok=True)\n                    sf.write(aug_path, aug_audio, sr)\n                    \n                    aug_entry = row.to_dict()\n                    aug_entry['audio_path'] = aug_path\n                    aug_entry['transcription'] = chunk['transcription']\n                    aug_entry['category'] = f\"{row['category']}_aug\"\n                    aug_entry['filename'] = aug_filename\n                    aug_entry['duration'] = len(aug_audio) / sr\n                    augmented_entries.append(aug_entry)\n        else:\n            # Normal processing for short audio\n            augmented_entries.append(row.to_dict())\n            \n            for aug_idx in range(augmentation_factor - 1):\n                audio, sr = librosa.load(row['audio_path'], sr=16000)\n                aug_audio = augment_audio(audio, sr)\n                \n                aug_filename = f\"aug_{aug_idx}_{Path(row['audio_path']).name}\"\n                aug_path = f\"/kaggle/working/augmented/{aug_filename}\"\n                os.makedirs(\"/kaggle/working/augmented\", exist_ok=True)\n                sf.write(aug_path, aug_audio, sr)\n                \n                augmented_entries.append({\n                    'audio_path': aug_path,\n                    'transcription': row['transcription'],\n                    'category': f\"{row['category']}_aug\",\n                    'filename': aug_filename,\n                    'duration': len(aug_audio) / sr\n                })\n    \n    return pd.DataFrame(augmented_entries)","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-11-06T09:34:41.647906Z","iopub.execute_input":"2025-11-06T09:34:41.648218Z","iopub.status.idle":"2025-11-06T09:34:41.659767Z","shell.execute_reply.started":"2025-11-06T09:34:41.648190Z","shell.execute_reply":"2025-11-06T09:34:41.659042Z"}},"outputs":[],"execution_count":null},{"cell_type":"code","source":"# Existing augmentation code...\ndf_augmented = create_augmented_dataset(df, augmentation_factor=2)\n\n# Add base_category for stratification (strips '_aug')\ndf_augmented['base_category'] = df_augmented['category'].str.replace('_aug', '')\n\n# Optional: Verify counts on base_category (all should now >=2)\nprint(\"üìä Base Category Counts:\")\nprint(df_augmented['base_category'].value_counts())\nprint(\"\\nBase Categories with <2 samples:\", df_augmented['base_category'].value_counts()[df_augmented['base_category'].value_counts() < 2].index.tolist())","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-11-06T09:34:41.660549Z","iopub.execute_input":"2025-11-06T09:34:41.660809Z","iopub.status.idle":"2025-11-06T09:35:19.319096Z","shell.execute_reply.started":"2025-11-06T09:34:41.660787Z","shell.execute_reply":"2025-11-06T09:35:19.318169Z"}},"outputs":[],"execution_count":null},{"cell_type":"code","source":"len(df_augmented)","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-11-06T09:35:19.319948Z","iopub.execute_input":"2025-11-06T09:35:19.320244Z","iopub.status.idle":"2025-11-06T09:35:19.325667Z","shell.execute_reply.started":"2025-11-06T09:35:19.320217Z","shell.execute_reply":"2025-11-06T09:35:19.324877Z"}},"outputs":[],"execution_count":null},{"cell_type":"markdown","source":"# train test split","metadata":{}},{"cell_type":"code","source":"from sklearn.model_selection import train_test_split\n\n# 80% train, 10% val, 10% test\ntrain_df, temp_df = train_test_split(df_augmented, test_size=0.2, random_state=42, \n                                      stratify=df_augmented['base_category'])\nval_df, test_df = train_test_split(temp_df, test_size=0.5, random_state=42,\n                                    stratify=temp_df['base_category'])\n\nprint(f\"üìä Dataset Split:\")\nprint(f\"   Train: {len(train_df)} samples ({len(train_df)/len(df_augmented)*100:.1f}%)\")\nprint(f\"   Val:   {len(val_df)} samples ({len(val_df)/len(df_augmented)*100:.1f}%)\")\nprint(f\"   Test:  {len(test_df)} samples ({len(test_df)/len(df_augmented)*100:.1f}%)\\n\")","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-11-06T09:35:23.264976Z","iopub.execute_input":"2025-11-06T09:35:23.265268Z","iopub.status.idle":"2025-11-06T09:35:23.277960Z","shell.execute_reply.started":"2025-11-06T09:35:23.265249Z","shell.execute_reply":"2025-11-06T09:35:23.277284Z"}},"outputs":[],"execution_count":null},{"cell_type":"markdown","source":"# loading the model","metadata":{}},{"cell_type":"code","source":"print(\"üì• Loading Whisper Large-v3...\\n\")\n\nmodel_name = \"openai/whisper-large-v3\"\n\nfeature_extractor = WhisperFeatureExtractor.from_pretrained(model_name)\ntokenizer = WhisperTokenizer.from_pretrained(model_name, language=\"English\", task=\"transcribe\")\nprocessor = WhisperProcessor.from_pretrained(model_name, language=\"English\", task=\"transcribe\")\n\nbnb_config = BitsAndBytesConfig(\n    load_in_4bit=True,\n    bnb_4bit_quant_type=\"nf4\",\n    bnb_4bit_compute_dtype=torch.float16,\n    bnb_4bit_use_double_quant=True  # Optional: nested quantization for extra memory savings\n)\n\nmodel = WhisperForConditionalGeneration.from_pretrained(\n    model_name,\n    quantization_config=bnb_config,\n    device_map=\"auto\"\n)\n\n\nmodel.generation_config.language = \"english\"\nmodel.generation_config.task = \"transcribe\"\nmodel.generation_config.forced_decoder_ids = None\n\nprint(\"‚úÖ Model loaded!\\n\")","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-11-06T09:35:26.499911Z","iopub.execute_input":"2025-11-06T09:35:26.500765Z","iopub.status.idle":"2025-11-06T09:35:52.878367Z","shell.execute_reply.started":"2025-11-06T09:35:26.500738Z","shell.execute_reply":"2025-11-06T09:35:52.877700Z"}},"outputs":[],"execution_count":null},{"cell_type":"markdown","source":"# applying lora confriguation","metadata":{}},{"cell_type":"code","source":"print(\"üîß Applying LoRA configuration...\\n\")\n# Prepare model for LoRA training\nmodel = prepare_model_for_kbit_training(model)\n# Optionally freeze the encoder to focus LoRA on decoder only (for efficiency)\nmodel.model.encoder.requires_grad_(False)\n# LoRA Configuration for Whisper\nlora_config = LoraConfig(\n    r=32,                          # LoRA rank (higher = more parameters, better quality)\n    lora_alpha=64,                 # LoRA scaling factor\n    target_modules=[               # Use simple suffixes to match all relevant layers (applies to both encoder/decoder)\n        \"q_proj\",\n        \"v_proj\",\n        \"k_proj\",\n        \"out_proj\",\n        \"fc1\",\n        \"fc2\"\n    ],\n    lora_dropout=0.05,             # Dropout for LoRA layers\n    bias=\"none\",                   # Don't train biases\n    # task_type=\"SEQ_2_SEQ_LM\"       # Critical fix: Use seq2seq for Whisper\n)\n# Apply LoRA to model\nmodel = get_peft_model(model, lora_config)\n# Print trainable parameters\ntrainable_params = sum(p.numel() for p in model.parameters() if p.requires_grad)\ntotal_params = sum(p.numel() for p in model.parameters())\nprint(f\"{'='*60}\")\nprint(f\"üìä LoRA Model Statistics:\")\nprint(f\"   Trainable params: {trainable_params:,} ({trainable_params/total_params*100:.2f}%)\")\nprint(f\"   Total params: {total_params:,}\")\nprint(f\"   Memory reduction: ~{100 - (trainable_params/total_params*100):.1f}%\")\nprint(f\"{'='*60}\\n\")","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-11-06T09:35:55.994166Z","iopub.execute_input":"2025-11-06T09:35:55.994477Z","iopub.status.idle":"2025-11-06T09:35:57.450191Z","shell.execute_reply.started":"2025-11-06T09:35:55.994457Z","shell.execute_reply":"2025-11-06T09:35:57.449343Z"}},"outputs":[],"execution_count":null},{"cell_type":"markdown","source":"# Prepare Dataset for Training (SAME AS BEFORE)\n","metadata":{}},{"cell_type":"code","source":"def prepare_dataset_entry(batch):\n    \"\"\"\n    Prepare dataset entry with token length validation.\n    \"\"\"\n    audio, sr = librosa.load(batch[\"audio_path\"], sr=16000)\n    \n    batch[\"input_features\"] = processor.feature_extractor(\n        audio, sampling_rate=16000\n    ).input_features[0]\n    \n    # Tokenize with truncation as safety measure\n    labels = tokenizer(\n        batch[\"transcription\"],\n        truncation=True,\n        max_length=448  # Whisper's max token length\n    ).input_ids\n    \n    batch[\"labels\"] = labels\n    \n    # Warning if truncation occurred\n    if len(labels) >= 448:\n        print(f\"‚ö†Ô∏è  Truncated labels for: {batch.get('audio_path', 'unknown')} (originally {len(labels)} tokens)\")\n    \n    return batch\n\n\ndef df_to_dataset(df):\n    return Dataset.from_dict({\n        \"audio_path\": df[\"audio_path\"].tolist(),\n        \"transcription\": df[\"transcription\"].tolist()\n    })\n\nprint(\"üîÑ Processing dataset...\")\ntrain_dataset = df_to_dataset(train_df).map(prepare_dataset_entry, remove_columns=[\"audio_path\", \"transcription\"])\nval_dataset = df_to_dataset(val_df).map(prepare_dataset_entry, remove_columns=[\"audio_path\", \"transcription\"])\ntest_dataset = df_to_dataset(test_df).map(prepare_dataset_entry, remove_columns=[\"audio_path\", \"transcription\"])\n\nprint(\"‚úÖ Dataset ready!\\n\")\n","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-11-06T09:36:04.820903Z","iopub.execute_input":"2025-11-06T09:36:04.821491Z","iopub.status.idle":"2025-11-06T09:36:23.166694Z","shell.execute_reply.started":"2025-11-06T09:36:04.821465Z","shell.execute_reply":"2025-11-06T09:36:23.165801Z"}},"outputs":[],"execution_count":null},{"cell_type":"markdown","source":"# data collateral","metadata":{}},{"cell_type":"code","source":"from dataclasses import dataclass\nfrom typing import Any, Dict, List, Union\nimport torch\n\n@dataclass\nclass DataCollatorSpeechSeq2SeqWithPadding:\n    processor: Any\n    decoder_start_token_id: int\n\n    def __call__(self, features: List[Dict[str, Union[List[int], torch.Tensor]]]) -> Dict[str, torch.Tensor]:\n        # Extract input features from batch\n        input_features = [{\"input_features\": feature[\"input_features\"]} for feature in features]\n        batch = self.processor.feature_extractor.pad(input_features, return_tensors=\"pt\")\n        \n        # Extract labels\n        label_features = [{\"input_ids\": feature[\"labels\"]} for feature in features]\n        labels_batch = self.processor.tokenizer.pad(label_features, return_tensors=\"pt\")\n        \n        # Replace padding with -100 to ignore loss correctly\n        labels = labels_batch[\"input_ids\"].masked_fill(labels_batch.attention_mask.ne(1), -100)\n        \n        # If bos token is prepended in previous tokenization step, remove it\n        if (labels[:, 0] == self.decoder_start_token_id).all().cpu().item():\n            labels = labels[:, 1:]\n        \n        batch[\"labels\"] = labels\n        \n        return batch  # ‚úÖ Return the full batch dict\n\ndata_collator = DataCollatorSpeechSeq2SeqWithPadding(\n    processor=processor,\n    decoder_start_token_id=model.config.decoder_start_token_id,\n)\n","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-11-06T09:36:23.167842Z","iopub.execute_input":"2025-11-06T09:36:23.168063Z","iopub.status.idle":"2025-11-06T09:36:23.175257Z","shell.execute_reply.started":"2025-11-06T09:36:23.168047Z","shell.execute_reply":"2025-11-06T09:36:23.174448Z"}},"outputs":[],"execution_count":null},{"cell_type":"markdown","source":"# evaluation metrics","metadata":{}},{"cell_type":"code","source":"# Metric computation\nmetric = evaluate.load(\"wer\")\n\ndef compute_metrics(pred):\n    pred_ids = pred.predictions\n    label_ids = pred.label_ids\n\n    label_ids[label_ids == -100] = tokenizer.pad_token_id\n\n    pred_str = tokenizer.batch_decode(pred_ids, skip_special_tokens=True)\n    label_str = tokenizer.batch_decode(label_ids, skip_special_tokens=True)\n\n    wer = 100 * metric.compute(predictions=pred_str, references=label_str)\n\n    return {\"wer\": wer}","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-11-06T09:36:31.017102Z","iopub.execute_input":"2025-11-06T09:36:31.017415Z","iopub.status.idle":"2025-11-06T09:36:31.820580Z","shell.execute_reply.started":"2025-11-06T09:36:31.017393Z","shell.execute_reply":"2025-11-06T09:36:31.820051Z"}},"outputs":[],"execution_count":null},{"cell_type":"markdown","source":"# Training Configuration (OPTIMIZED FOR LORA)\n","metadata":{}},{"cell_type":"code","source":"\n# Enable gradient checkpointing for memory efficiency\nmodel.gradient_checkpointing_enable()\nmodel.config.use_cache = False\n\n# Training arguments\ntraining_args = Seq2SeqTrainingArguments(\n    output_dir=\"./whisper-large-v3-malaysian-lora\",\n    per_device_train_batch_size=4,      # Reduce if OOM\n    gradient_accumulation_steps=4,       # Adjust based on memory\n    learning_rate=1e-3,\n    warmup_steps=50,\n    max_steps=500,\n    gradient_checkpointing=True,\n    fp16=True,\n    eval_strategy=\"steps\",\n    per_device_eval_batch_size=4,\n    predict_with_generate=True,\n    generation_max_length=225,\n    save_steps=100,\n    eval_steps=100,\n    logging_steps=10,\n    report_to=[\"tensorboard\"],\n    load_best_model_at_end=True,\n    metric_for_best_model=\"wer\",\n    greater_is_better=False,\n    push_to_hub=False,\n    remove_unused_columns=False,\n    dataloader_num_workers=2,\n)\n","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-11-06T09:36:42.941614Z","iopub.execute_input":"2025-11-06T09:36:42.941893Z","iopub.status.idle":"2025-11-06T09:36:42.992796Z","shell.execute_reply.started":"2025-11-06T09:36:42.941875Z","shell.execute_reply":"2025-11-06T09:36:42.992202Z"}},"outputs":[],"execution_count":null},{"cell_type":"markdown","source":"# Initialize trainer","metadata":{}},{"cell_type":"code","source":"# Verify your dataset structure\nprint(\"Checking dataset structure:\")\nprint(\"Train sample keys:\", train_dataset[0].keys())\nprint(\"Input features shape:\", np.array(train_dataset[0][\"input_features\"]).shape)\nprint(\"Labels sample:\", train_dataset[0][\"labels\"][:10])\n\n# Test data collator\nprint(\"\\nTesting data collator:\")\nsample_batch = [train_dataset[0], train_dataset[1]]\n\ncollated = data_collator(sample_batch)\nprint(\"Collated batch keys:\", collated.keys())\nprint(\"Input features shape:\", collated[\"input_features\"].shape)\nprint(\"Labels shape:\", collated[\"labels\"].shape)\n\n# ‚úÖ CRITICAL FIX: Pass feature_extractor as tokenizer\ntrainer = Seq2SeqTrainer(\n    args=training_args,\n    model=model,\n    train_dataset=train_dataset,\n    eval_dataset=val_dataset,\n    data_collator=data_collator,\n    compute_metrics=compute_metrics,\n    tokenizer=processor.feature_extractor,  # ‚úÖ Use feature_extractor, NOT tokenizer\n)\n\nprint(\"üöÄ Starting LoRA training...\\n\")\nprint(\"=\"*60)\n\n# Clear GPU cache before training\ntorch.cuda.empty_cache()\n\ntrainer.train()\n\nprint(\"\\n\" + \"=\"*60)\nprint(\"‚úÖ Training complete!\")\nprint(\"=\"*60)","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-11-06T09:36:45.799215Z","iopub.execute_input":"2025-11-06T09:36:45.799887Z"}},"outputs":[],"execution_count":null},{"cell_type":"markdown","source":"# evaluation on test ","metadata":{}},{"cell_type":"code","source":"print(\"\\nüß™ Evaluating on test set...\")\ntest_results = trainer.evaluate(test_dataset)\n\nprint(f\"\\n{'='*60}\")\nprint(f\"üìä Final Test Results:\")\nprint(f\"   WER: {test_results['eval_wer']:.2f}%\")\nprint(f\"{'='*60}\\n\")\n","metadata":{"trusted":true},"outputs":[],"execution_count":null},{"cell_type":"code","source":"# STEP 14: Save LoRA Adapters (IMPORTANT!)\n# ============================================================================\n\nprint(\"üíæ Saving LoRA adapters...\")\n\n# Save only LoRA weights (very small file!)\nmodel.save_pretrained(\"./whisper-large-v3-lora-adapters\")\nprocessor.save_pretrained(\"./whisper-large-v3-lora-adapters\")\n\nprint(\"‚úÖ LoRA adapters saved! (~10-50MB instead of 3GB)\")\n","metadata":{"trusted":true},"outputs":[],"execution_count":null},{"cell_type":"markdown","source":"","metadata":{}},{"cell_type":"code","source":"# STEP 15: Merge LoRA weights with base model (Optional)\n# ============================================================================\n\nprint(\"\\nüîÄ Merging LoRA weights into full model...\")\n\n# Merge and save full model\nmerged_model = model.merge_and_unload()\nmerged_model.save_pretrained(\"./whisper-large-v3-malaysian-merged\")\nprocessor.save_pretrained(\"./whisper-large-v3-malaysian-merged\")\n\nprint(\"‚úÖ Merged model saved!\")\n","metadata":{"trusted":true},"outputs":[],"execution_count":null},{"cell_type":"code","source":"print(\"\\nüéØ Testing predictions...\\n\")\n\nfrom transformers import pipeline\n\npipe = pipeline(\n    \"automatic-speech-recognition\",\n    model=\"./whisper-large-v3-malaysian-merged\",\n    tokenizer=processor.tokenizer,\n    feature_extractor=processor.feature_extractor,\n    device=0 if torch.cuda.is_available() else -1\n)\n\ntest_samples = test_df.sample(min(5, len(test_df)))\n\nfor idx, row in test_samples.iterrows():\n    prediction = pipe(row['audio_path'])[\"text\"]\n    print(f\"File: {row['filename']}\")\n    print(f\"Expected: {row['transcription']}\")\n    print(f\"Predicted: {prediction}\")\n    print(f\"-\" * 40)\n\nprint(\"\\n‚úÖ LoRA fine-tuning complete!\")\nprint(\"\\nüì¶ You now have:\")\nprint(\"   1. LoRA adapters: ./whisper-large-v3-lora-adapters (~10-50MB)\")\nprint(\"   2. Merged model: ./whisper-large-v3-malaysian-merged (~3GB)\")\n","metadata":{"trusted":true},"outputs":[],"execution_count":null}]}